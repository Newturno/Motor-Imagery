{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import pywt\n",
    "from mne.time_frequency import tfr_multitaper\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.cuda as cuda\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "\n",
    "from utils import create_dataloader,train,bandpower\n",
    "from dataset import EEG\n",
    "import wandb\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "# Now do your import\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/pongkornsettasompop/Desktop/work/Motor-Imagery/EEG-python/dataset/recorded_EEG\n",
      "/Users/pongkornsettasompop/Desktop/work/Motor-Imagery/EEG-python/dataset/recorded_EEG\n",
      "Raw done\n",
      "(9, 237454)\n",
      "250.0\n"
     ]
    }
   ],
   "source": [
    "# home directory + datasets folder\n",
    "#C:\\Users\\Pongk\\Desktop\\Work\\mi-project\\EEG-python\\dataset\\recorded_EEG\n",
    "path = RECORED_PATH\n",
    "#path = \"/root/EEG_Model/dataset/finetune_EEG/\"\n",
    "#subject to run\n",
    "left_runs = [3,5,7,9,11]\n",
    "right_runs = [4,6,8,10,12]\n",
    "#[3,5,7,9,11]\n",
    "#[4,6,8,10,12]\n",
    "#runs = [1,2,3,4,12,13,14,15,16,17]\n",
    "#runs = [1]\n",
    "#runs = [7,8,9,10]\n",
    "subjects = [21]\n",
    "#recorded eeg class\n",
    "left_eeg = EEG(path, subjects, left_runs)\n",
    "raw=left_eeg.data_to_raw()\n",
    "right_eeg = EEG(path, subjects, right_runs)\n",
    "right_raw = right_eeg.data_to_raw()\n",
    "\n",
    "print(\"Raw done\")\n",
    "data, sf = raw.get_data(), raw.info['sfreq']\n",
    "\n",
    "print(data.shape)\n",
    "print(sf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw = eeg.set_reference(raw,['CZ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n"
     ]
    }
   ],
   "source": [
    "#pick channel\n",
    "# print(raw.info['ch_names'])\n",
    "raw = left_eeg.pickChannel(raw,['T3','C3','C4','T4','STIM MARKERS'])\n",
    "right_raw = right_eeg.pickChannel(right_raw,['T3','C3','C4','T4','STIM MARKERS'])\n",
    "# print(raw.info['ch_names'])\n",
    "# print(len(raw.info['ch_names']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[853, 5607, 10361, 15114, 19868, 24622, 29375, 34128, 38882, 43635, 48366, 53120, 57873, 62627, 67381, 72134, 76888, 81642, 86396, 91150, 95882, 100623, 105379, 110023, 114773, 119528, 124286, 129039, 133793, 138547, 143279, 148033, 152786, 157539, 162293, 167046, 171800, 176553, 181306, 186059, 190790, 195544, 200297, 205051, 209804, 214558, 219311, 224065, 228819, 233574]\n",
      "[3230, 7984, 12737, 17490, 22245, 26998, 31751, 36505, 41258, 46012, 50743, 55496, 60251, 65004, 69758, 74511, 79265, 84019, 88773, 93526, 98258, 102984, 107670, 112401, 117154, 121909, 126656, 131416, 136169, 140924, 145656, 150409, 155163, 159916, 164670, 169423, 174176, 178930, 183683, 188436, 193167, 197920, 202674, 207428, 212181, 216934, 221688, 226442, 231198, 235952]\n",
      "(100, 4, 1750) (100,)\n"
     ]
    }
   ],
   "source": [
    "X_l,y_l = left_eeg.raw_preprocess(raw,event_id=[1.0],rest_stage=False)\n",
    "#print(X_l.shape,y_l.shape)\n",
    "X_r,y_r = right_eeg.raw_preprocess(right_raw,event_id=[2.0],rest_stage=False)\n",
    "X = np.concatenate((X_l,X_r),axis=0)\n",
    "y = np.concatenate((y_l,y_r),axis=0)\n",
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective window size : 1.024 (s)\n",
      "Average band power2.5272651796138397\n",
      "Multitaper band0.1512771642244757\n"
     ]
    }
   ],
   "source": [
    "#Feature extraction\n",
    "bp = bandpower(X, sf, [8, 13])\n",
    "mul_bp = bandpower(X,sf,[8,13],method='multitaper')\n",
    "print('Average band power'+ str(bp))\n",
    "print('Multitaper band'+ str(mul_bp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 4, 1750)\n",
      "[ 3.68788451  3.34832833  2.80377089 ... -1.10874719 -2.28671036\n",
      " -3.39157687]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(X[0,0,:])\n",
    "print(y)\n",
    "\n",
    "X = left_eeg.apply_baseline(X)\n",
    "#X = np.transpose(X,(0,2,1))\n",
    "#print(X[0,0,:])\n",
    "#print(np.transpose(X,(0,2,1)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epochs=eeg.epochs(raw,tmin=-2,tmax=5,baseline=(-2,0))\n",
    "print(len(epochs.times))\n",
    "#X = X[:, :,np.newaxis,:]\n",
    "X, y = eeg.get_X_y(epochs)\n",
    "#(250*5)\n",
    "#normal version \n",
    "#X = X[:,:,(250*3):]\n",
    "\n",
    "#new version\n",
    "#X = X[:,:,(250*8):(250*13)]\n",
    "\n",
    "print(X.shape)\n",
    "print(type(X))\n",
    "#new_X = np.concatenate((fixation_X,imagine_X),axis=2)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size (70, 4, 1750) (70,)\n",
      "Test size (30, 4, 1750) (30,)\n",
      "[1 0 1 1 0 1 1 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0\n",
      " 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 0 1 1 1]\n",
      "[0 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 1 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "#Old version use X. New version use new_X\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,stratify=y)\n",
    "print('Train size',X_train.shape, y_train.shape)\n",
    "print('Test size',X_test.shape, y_test.shape)\n",
    "print(y_train)\n",
    "print(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1750\n",
    "\n",
    "train_loader = create_dataloader(X_train, y_train, batch_size=batch_size)\n",
    "test_loader = create_dataloader(X_test, y_test, batch_size=batch_size)\n",
    "\n",
    "num_step =math.ceil(len(train_loader.dataset) / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wand setup\n",
    "#\"weightname\":\"S12_3-9_fir8-14\"\n",
    "#name=f\"CNN_S12_3-9_fir8-14\",\n",
    "wandb.login()\n",
    "wand = wandb.init(\n",
    "      # Set the project where this run will be logged\n",
    "      project=\"Motor-Imagery-New\", \n",
    "      # We pass a run name (otherwise itâ€™ll be randomly assigned, like sunshine-lollypop-10)\n",
    "      name=f\"CNN_S21_iir_LFRF_4ch\", \n",
    "      # Track hyperparameters and run metadata\n",
    "      # 0.0000001\n",
    "      config={\n",
    "      \"learning_rate\": 0.0000001,\n",
    "      \"architecture\": \"ConvNet\",\n",
    "      \"dataset\": \"Recorded\",\n",
    "      \"epochs\": 100000,\n",
    "      \"weightname\":\"S21_CNN_iir_LFRF_4ch\",\n",
    "      \"num_step_per_epoch\" : num_step, \n",
    "      }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import ConvNet,CNN2D,gamenet,ConvNet2\n",
    "from torchsummary import summary\n",
    "from transformer import Transformer\n",
    "net = ConvNet2()\n",
    "\n",
    "# device = 'cuda'\n",
    "# sequence_len=1750 # sequence length of time series\n",
    "# max_len=5000 # max time series sequence length \n",
    "# n_head = 4 # number of attention head\n",
    "# n_layer = 2 # number of encoder layer\n",
    "# drop_prob = 0.1\n",
    "# d_model = 200 # number of dimension ( for positional embedding)\n",
    "# ffn_hidden = 512 # size of hidden layer before classification \n",
    "# in_features = 3 # for univariate time series (1d), it must be adjusted for 1. \n",
    "# n_classes = 2\n",
    "# net =  Transformer( in_features=in_features,\n",
    "#                      d_model=d_model,\n",
    "#                      details=False,\n",
    "#                      n_head=n_head,\n",
    "#                      max_len=max_len,\n",
    "#                      seq_len=sequence_len,\n",
    "#                      ffn_hidden=ffn_hidden,\n",
    "#                      n_layers=n_layer,\n",
    "#                      drop_prob=drop_prob,\n",
    "#                      n_classes=n_classes,\n",
    "#                      device=device\n",
    "#                      )\n",
    "# #summary(net, (2, 641),32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import train\n",
    "config = wand.config\n",
    "optimizer = optim.Adam(net.parameters(), lr=config.learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = 'cuda'\n",
    "\n",
    "train_loss,valid_loss,train_accuracy,valid_accuracy =train(\n",
    "    model = net,\n",
    "    loader_train = train_loader,\n",
    "    loader_test = test_loader,\n",
    "    vail_loader = None,\n",
    "    optimizer = optimizer  ,\n",
    "    criterion = criterion ,\n",
    "    device = 'cuda',\n",
    "    wand = wand\n",
    ")\n",
    "\n",
    "\n",
    "wandb.alert(\n",
    "            title='Finish',\n",
    "            text=f'Finishing training',\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torchsummary import summary\n",
    "from transformer import Transformer\n",
    " \n",
    "\n",
    "sequence_len=1750 # sequence length of time series\n",
    "max_len=5000 # max time series sequence length \n",
    "n_head = 4 # number of attention head\n",
    "n_layer = 2 # number of encoder layer\n",
    "drop_prob = 0.1\n",
    "d_model = 200 # number of dimension ( for positional embedding)\n",
    "ffn_hidden = 512 # size of hidden layer before classification \n",
    "in_features = 3 # for univariate time series (1d), it must be adjusted for 1. \n",
    "n_classes = 2\n",
    "model =  Transformer( in_features=in_features,\n",
    "                     d_model=d_model,\n",
    "                     details=False,\n",
    "                     n_head=n_head,\n",
    "                     max_len=max_len,\n",
    "                     seq_len=sequence_len,\n",
    "                     ffn_hidden=ffn_hidden,\n",
    "                     n_layers=n_layer,\n",
    "                     drop_prob=drop_prob,\n",
    "                     n_classes=n_classes,\n",
    "                     device=device)\n",
    "\n",
    "batch_size = 555\n",
    "\n",
    "#summary(net, (2, 641),32)\n",
    "#summary(model, input_size=(batch_size,sequence_len,feature))\n",
    "\n",
    "input_ = torch.from_numpy(np.empty((batch_size,sequence_len,in_features))).float()\n",
    "\n",
    "model(input_)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "71ee62090f476f7f208daa0d546a5a64db59508b1a22febc715667ce49424855"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
