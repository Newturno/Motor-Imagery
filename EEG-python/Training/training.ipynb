{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import pywt\n",
    "from mne.time_frequency import tfr_multitaper\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.cuda as cuda\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "\n",
    "from utils import create_dataloader,train,bandpower\n",
    "from dataset import EEG\n",
    "import wandb\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "# Now do your import\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/pongkornsettasompop/Desktop/work/Motor-Imagery/EEG-python/dataset/recorded_EEG\n",
      "/Users/pongkornsettasompop/Desktop/work/Motor-Imagery/EEG-python/dataset/recorded_EEG\n",
      "Raw done\n",
      "(9, 30184)\n",
      "250.0\n"
     ]
    }
   ],
   "source": [
    "# home directory + datasets folder\n",
    "#C:\\Users\\Pongk\\Desktop\\Work\\mi-project\\EEG-python\\dataset\\recorded_EEG\n",
    "path = RECORED_PATH\n",
    "#path = \"/root/EEG_Model/dataset/finetune_EEG/\"\n",
    "#subject to run\n",
    "left_runs = [13]\n",
    "right_runs = [4,6,8,10,12]\n",
    "#[2,14]\n",
    "#[3,5,7,9,11]\n",
    "#[4,6,8,10,12]\n",
    "#runs = [1,2,3,4,12,13,14,15,16,17]\n",
    "#runs = [1]\n",
    "#runs = [7,8,9,10]\n",
    "subjects = [21]\n",
    "#recorded eeg class\n",
    "left_eeg = EEG(path, subjects, left_runs)\n",
    "raw=left_eeg.data_to_raw()\n",
    "right_eeg = EEG(path, subjects, right_runs)\n",
    "right_raw = right_eeg.data_to_raw()\n",
    "\n",
    "print(\"Raw done\")\n",
    "data, sf = raw.get_data(), raw.info['sfreq']\n",
    "\n",
    "print(data.shape)\n",
    "print(sf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw = eeg.set_reference(raw,['CZ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n"
     ]
    }
   ],
   "source": [
    "#pick channel\n",
    "# print(raw.info['ch_names'])\n",
    "raw = left_eeg.pickChannel(raw,['T3','C3','C4','T4','STIM MARKERS'])\n",
    "right_raw = right_eeg.pickChannel(right_raw,['T3','C3','C4','T4','STIM MARKERS'])\n",
    "# print(raw.info['ch_names'])\n",
    "# print(len(raw.info['ch_names']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[856, 3174, 5524, 7659, 9548, 11591, 13726, 15230, 16889, 18205, 19833, 21112, 22723, 23936, 24996, 25838, 26629, 27563, 28411, 29568]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 2, the array at index 0 has size 1750 and the array at index 19 has size 1116",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/pongkornsettasompop/Desktop/work/Motor-Imagery/EEG-python/Training/training.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/pongkornsettasompop/Desktop/work/Motor-Imagery/EEG-python/Training/training.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m X_l,y_l \u001b[39m=\u001b[39m left_eeg\u001b[39m.\u001b[39;49mraw_preprocess(raw,event_id\u001b[39m=\u001b[39;49m[\u001b[39m3.0\u001b[39;49m],rest_stage\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pongkornsettasompop/Desktop/work/Motor-Imagery/EEG-python/Training/training.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(X_l\u001b[39m.\u001b[39mshape,y_l\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pongkornsettasompop/Desktop/work/Motor-Imagery/EEG-python/Training/training.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(X_l[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/Desktop/work/Motor-Imagery/EEG-python/Training/dataset.py:131\u001b[0m, in \u001b[0;36mEEG.raw_preprocess\u001b[0;34m(self, raw, event_id, rest_stage)\u001b[0m\n\u001b[1;32m    129\u001b[0m     trialofintereted\u001b[39m.\u001b[39mappend(filtered)\n\u001b[1;32m    130\u001b[0m temp \u001b[39m=\u001b[39m [t[np\u001b[39m.\u001b[39mnewaxis, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m] \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m trialofintereted]\n\u001b[0;32m--> 131\u001b[0m data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mconcatenate(temp)\n\u001b[1;32m    132\u001b[0m y \u001b[39m=\u001b[39m eeg_df[eeg_df[\u001b[39m'\u001b[39m\u001b[39mSTIM MARKERS\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39misin(event_id)]\n\u001b[1;32m    133\u001b[0m y \u001b[39m=\u001b[39m y[\u001b[39m'\u001b[39m\u001b[39mSTIM MARKERS\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto_numpy()\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 2, the array at index 0 has size 1750 and the array at index 19 has size 1116"
     ]
    }
   ],
   "source": [
    "X_l,y_l = left_eeg.raw_preprocess(raw,event_id=[3.0],rest_stage=False)\n",
    "print(X_l.shape,y_l.shape)\n",
    "print(X_l[0].shape)\n",
    "bp = bandpower(X_l[0], sf, [8, 10])\n",
    "mul_bp = bandpower(X_l,sf,[8,13],method='multitaper')\n",
    "print('Average band power'+ str(bp))\n",
    "print('Multitaper band'+ str(mul_bp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_r,y_r = right_eeg.raw_preprocess(right_raw,event_id=[2.0],rest_stage=False)\n",
    "print(X_r.shape,y_l.shape)\n",
    "print(X_r[0].shape)\n",
    "bp = bandpower(X_r, sf, [12, 16])\n",
    "mul_bp = bandpower(X_r,sf,[8,13],method='multitaper')\n",
    "print('Average band power'+ str(bp))\n",
    "print('Multitaper band'+ str(mul_bp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_l,y_l = left_eeg.raw_preprocess(raw,event_id=[1.0],rest_stage=False)\n",
    "print(X_l.shape,y_l.shape)\n",
    "X_r,y_r = right_eeg.raw_preprocess(right_raw,event_id=[2.0],rest_stage=False)\n",
    "X = np.concatenate((X_l,X_r),axis=0)\n",
    "y = np.concatenate((y_l,y_r),axis=0)\n",
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature extraction\n",
    "bp = bandpower(X, sf, [8, 13])\n",
    "mul_bp = bandpower(X,sf,[8,13],method='multitaper')\n",
    "print('Average band power'+ str(bp))\n",
    "print('Multitaper band'+ str(mul_bp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(X[0,0,:])\n",
    "print(y)\n",
    "\n",
    "X = left_eeg.apply_baseline(X)\n",
    "#X = np.transpose(X,(0,2,1))\n",
    "#print(X[0,0,:])\n",
    "#print(np.transpose(X,(0,2,1)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epochs=eeg.epochs(raw,tmin=-2,tmax=5,baseline=(-2,0))\n",
    "print(len(epochs.times))\n",
    "#X = X[:, :,np.newaxis,:]\n",
    "X, y = eeg.get_X_y(epochs)\n",
    "#(250*5)\n",
    "#normal version \n",
    "#X = X[:,:,(250*3):]\n",
    "\n",
    "#new version\n",
    "#X = X[:,:,(250*8):(250*13)]\n",
    "\n",
    "print(X.shape)\n",
    "print(type(X))\n",
    "#new_X = np.concatenate((fixation_X,imagine_X),axis=2)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Old version use X. New version use new_X\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,stratify=y)\n",
    "print('Train size',X_train.shape, y_train.shape)\n",
    "print('Test size',X_test.shape, y_test.shape)\n",
    "print(y_train)\n",
    "print(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1750\n",
    "\n",
    "train_loader = create_dataloader(X_train, y_train, batch_size=batch_size)\n",
    "test_loader = create_dataloader(X_test, y_test, batch_size=batch_size)\n",
    "\n",
    "num_step =math.ceil(len(train_loader.dataset) / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wand setup\n",
    "#\"weightname\":\"S12_3-9_fir8-14\"\n",
    "#name=f\"CNN_S12_3-9_fir8-14\",\n",
    "wandb.login()\n",
    "wand = wandb.init(\n",
    "      # Set the project where this run will be logged\n",
    "      project=\"Motor-Imagery-New\", \n",
    "      # We pass a run name (otherwise itâ€™ll be randomly assigned, like sunshine-lollypop-10)\n",
    "      name=f\"CNN_S21_iir_LFRF_4ch\", \n",
    "      # Track hyperparameters and run metadata\n",
    "      # 0.0000001\n",
    "      config={\n",
    "      \"learning_rate\": 0.0000001,\n",
    "      \"architecture\": \"ConvNet\",\n",
    "      \"dataset\": \"Recorded\",\n",
    "      \"epochs\": 100000,\n",
    "      \"weightname\":\"S21_CNN_iir_LFRF_4ch\",\n",
    "      \"num_step_per_epoch\" : num_step, \n",
    "      }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import ConvNet,CNN2D,gamenet,ConvNet2\n",
    "from torchsummary import summary\n",
    "from transformer import Transformer\n",
    "net = ConvNet2()\n",
    "\n",
    "# device = 'cuda'\n",
    "# sequence_len=1750 # sequence length of time series\n",
    "# max_len=5000 # max time series sequence length \n",
    "# n_head = 4 # number of attention head\n",
    "# n_layer = 2 # number of encoder layer\n",
    "# drop_prob = 0.1\n",
    "# d_model = 200 # number of dimension ( for positional embedding)\n",
    "# ffn_hidden = 512 # size of hidden layer before classification \n",
    "# in_features = 3 # for univariate time series (1d), it must be adjusted for 1. \n",
    "# n_classes = 2\n",
    "# net =  Transformer( in_features=in_features,\n",
    "#                      d_model=d_model,\n",
    "#                      details=False,\n",
    "#                      n_head=n_head,\n",
    "#                      max_len=max_len,\n",
    "#                      seq_len=sequence_len,\n",
    "#                      ffn_hidden=ffn_hidden,\n",
    "#                      n_layers=n_layer,\n",
    "#                      drop_prob=drop_prob,\n",
    "#                      n_classes=n_classes,\n",
    "#                      device=device\n",
    "#                      )\n",
    "# #summary(net, (2, 641),32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import train\n",
    "config = wand.config\n",
    "optimizer = optim.Adam(net.parameters(), lr=config.learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = 'cuda'\n",
    "\n",
    "train_loss,valid_loss,train_accuracy,valid_accuracy =train(\n",
    "    model = net,\n",
    "    loader_train = train_loader,\n",
    "    loader_test = test_loader,\n",
    "    vail_loader = None,\n",
    "    optimizer = optimizer  ,\n",
    "    criterion = criterion ,\n",
    "    device = 'cuda',\n",
    "    wand = wand\n",
    ")\n",
    "\n",
    "\n",
    "wandb.alert(\n",
    "            title='Finish',\n",
    "            text=f'Finishing training',\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torchsummary import summary\n",
    "from transformer import Transformer\n",
    " \n",
    "\n",
    "sequence_len=1750 # sequence length of time series\n",
    "max_len=5000 # max time series sequence length \n",
    "n_head = 4 # number of attention head\n",
    "n_layer = 2 # number of encoder layer\n",
    "drop_prob = 0.1\n",
    "d_model = 200 # number of dimension ( for positional embedding)\n",
    "ffn_hidden = 512 # size of hidden layer before classification \n",
    "in_features = 3 # for univariate time series (1d), it must be adjusted for 1. \n",
    "n_classes = 2\n",
    "model =  Transformer( in_features=in_features,\n",
    "                     d_model=d_model,\n",
    "                     details=False,\n",
    "                     n_head=n_head,\n",
    "                     max_len=max_len,\n",
    "                     seq_len=sequence_len,\n",
    "                     ffn_hidden=ffn_hidden,\n",
    "                     n_layers=n_layer,\n",
    "                     drop_prob=drop_prob,\n",
    "                     n_classes=n_classes,\n",
    "                     device=device)\n",
    "\n",
    "batch_size = 555\n",
    "\n",
    "#summary(net, (2, 641),32)\n",
    "#summary(model, input_size=(batch_size,sequence_len,feature))\n",
    "\n",
    "input_ = torch.from_numpy(np.empty((batch_size,sequence_len,in_features))).float()\n",
    "\n",
    "model(input_)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "71ee62090f476f7f208daa0d546a5a64db59508b1a22febc715667ce49424855"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
