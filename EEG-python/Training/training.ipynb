{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import pywt\n",
    "from mne.time_frequency import tfr_multitaper\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.cuda as cuda\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "\n",
    "from utils import create_dataloader,train,bandpower\n",
    "from dataset import EEG\n",
    "import wandb\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "# Now do your import\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/pongkornsettasompop/Desktop/work/Motor-Imagery/EEG-python/dataset/recorded_EEG\n",
      "/Users/pongkornsettasompop/Desktop/work/Motor-Imagery/EEG-python/dataset/recorded_EEG\n",
      "Raw done\n",
      "(9, 237577)\n",
      "250.0\n"
     ]
    }
   ],
   "source": [
    "# home directory + datasets folder\n",
    "#C:\\Users\\Pongk\\Desktop\\Work\\mi-project\\EEG-python\\dataset\\recorded_EEG\n",
    "path = RECORED_PATH\n",
    "#path = \"/root/EEG_Model/dataset/finetune_EEG/\"\n",
    "#subject to run\n",
    "left_runs = [3,5,7,9,11]\n",
    "right_runs = [4,6,8,10,12]\n",
    "#runs = [1,2,3,4,12,13,14,15,16,17]\n",
    "#runs = [1]\n",
    "#runs = [7,8,9,10]\n",
    "subjects = [21]\n",
    "#recorded eeg class\n",
    "left_eeg = EEG(path, subjects, left_runs)\n",
    "raw=left_eeg.data_to_raw()\n",
    "right_eeg = EEG(path, subjects, right_runs)\n",
    "right_raw = right_eeg.data_to_raw()\n",
    "\n",
    "print(\"Raw done\")\n",
    "data, sf = raw.get_data(), raw.info['sfreq']\n",
    "\n",
    "print(data.shape)\n",
    "print(sf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw = eeg.set_reference(raw,['CZ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n"
     ]
    }
   ],
   "source": [
    "#pick channel\n",
    "# print(raw.info['ch_names'])\n",
    "raw = left_eeg.pickChannel(raw,['T3','C3','C4','T4','STIM MARKERS'])\n",
    "right_raw = right_eeg.pickChannel(right_raw,['T3','C3','C4','T4','STIM MARKERS'])\n",
    "# print(raw.info['ch_names'])\n",
    "# print(len(raw.info['ch_names']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[853, 5606, 10361, 15114, 19867, 24621, 29375, 34127, 38880, 43634, 48350, 53117, 57869, 62616, 67377, 72131, 76884, 81639, 86392, 91145, 95884, 100638, 105392, 110146, 114900, 119653, 124407, 129161, 133915, 138668, 143400, 148153, 152907, 157661, 162414, 167167, 171922, 176675, 181429, 186182, 190914, 195668, 200420, 205176, 209930, 214683, 219437, 224191, 228944, 233697]\n",
      "[3230, 7984, 12737, 17490, 22245, 26998, 31751, 36505, 41258, 46012, 50743, 55496, 60251, 65004, 69758, 74511, 79265, 84019, 88773, 93526, 98258, 102984, 107670, 112401, 117154, 121909, 126656, 131416, 136169, 140924, 145656, 150409, 155163, 159916, 164670, 169423, 174176, 178930, 183683, 188436, 193167, 197920, 202674, 207428, 212181, 216934, 221688, 226442, 231198, 235952]\n",
      "(100, 4, 1750) (100,)\n"
     ]
    }
   ],
   "source": [
    "X_l,y_l = left_eeg.raw_preprocess(raw,event_id=[1.0],rest_stage=False)\n",
    "#print(X_l.shape,y_l.shape)\n",
    "X_r,y_r = right_eeg.raw_preprocess(right_raw,event_id=[2.0],rest_stage=False)\n",
    "X = np.concatenate((X_l,X_r),axis=0)\n",
    "y = np.concatenate((y_l,y_r),axis=0)\n",
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective window size : 1.024 (s)\n",
      "Average band power2.743614775032741\n",
      "Multitaper band0.165236882746269\n"
     ]
    }
   ],
   "source": [
    "#Feature extraction\n",
    "bp = bandpower(X, sf, [8, 13])\n",
    "mul_bp = bandpower(X,sf,[8,13],method='multitaper')\n",
    "print('Average band power'+ str(bp))\n",
    "print('Multitaper band'+ str(mul_bp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 4, 1750)\n",
      "[ 0.2086808   0.79441476  1.28208985 ... -0.73054999 -0.36940385\n",
      " -0.00375888]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(X[0,0,:])\n",
    "print(y)\n",
    "\n",
    "X = left_eeg.apply_baseline(X)\n",
    "#X = np.transpose(X,(0,2,1))\n",
    "#print(X[0,0,:])\n",
    "#print(np.transpose(X,(0,2,1)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epochs=eeg.epochs(raw,tmin=-2,tmax=5,baseline=(-2,0))\n",
    "print(len(epochs.times))\n",
    "#X = X[:, :,np.newaxis,:]\n",
    "X, y = eeg.get_X_y(epochs)\n",
    "#(250*5)\n",
    "#normal version \n",
    "#X = X[:,:,(250*3):]\n",
    "\n",
    "#new version\n",
    "#X = X[:,:,(250*8):(250*13)]\n",
    "\n",
    "print(X.shape)\n",
    "print(type(X))\n",
    "#new_X = np.concatenate((fixation_X,imagine_X),axis=2)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size (70, 4, 1750) (70,)\n",
      "Test size (30, 4, 1750) (30,)\n",
      "[1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 0 1 0\n",
      " 0 1 0 0 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 0 0 1]\n",
      "[0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 0 1 0 1 0 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "#Old version use X. New version use new_X\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,stratify=y)\n",
    "print('Train size',X_train.shape, y_train.shape)\n",
    "print('Test size',X_test.shape, y_test.shape)\n",
    "print(y_train)\n",
    "print(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1750\n",
    "\n",
    "train_loader = create_dataloader(X_train, y_train, batch_size=batch_size)\n",
    "test_loader = create_dataloader(X_test, y_test, batch_size=batch_size)\n",
    "\n",
    "num_step =math.ceil(len(train_loader.dataset) / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpongkorn-set\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/pongkornsettasompop/Desktop/work/Motor-Imagery/EEG-python/Training/wandb/run-20231130_122402-kqk7ekx0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pongkorn-set/Motor-Imagery-New/runs/kqk7ekx0' target=\"_blank\">CNN_S21_iir_LHRF_4ch</a></strong> to <a href='https://wandb.ai/pongkorn-set/Motor-Imagery-New' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/pongkorn-set/Motor-Imagery-New' target=\"_blank\">https://wandb.ai/pongkorn-set/Motor-Imagery-New</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/pongkorn-set/Motor-Imagery-New/runs/kqk7ekx0' target=\"_blank\">https://wandb.ai/pongkorn-set/Motor-Imagery-New/runs/kqk7ekx0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#wand setup\n",
    "#\"weightname\":\"S12_3-9_fir8-14\"\n",
    "#name=f\"CNN_S12_3-9_fir8-14\",\n",
    "wandb.login()\n",
    "wand = wandb.init(\n",
    "      # Set the project where this run will be logged\n",
    "      project=\"Motor-Imagery-New\", \n",
    "      # We pass a run name (otherwise itâ€™ll be randomly assigned, like sunshine-lollypop-10)\n",
    "      name=f\"CNN_S21_iir_LHRF_4ch\", \n",
    "      # Track hyperparameters and run metadata\n",
    "      # 0.0000001\n",
    "      config={\n",
    "      \"learning_rate\": 0.0000001,\n",
    "      \"architecture\": \"ConvNet\",\n",
    "      \"dataset\": \"Recorded\",\n",
    "      \"epochs\": 100000,\n",
    "      \"weightname\":\"S21_CNN_iir_LHRF_4ch\",\n",
    "      \"num_step_per_epoch\" : num_step, \n",
    "      }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import ConvNet,CNN2D,gamenet,ConvNet2\n",
    "from torchsummary import summary\n",
    "from transformer import Transformer\n",
    "net = ConvNet2()\n",
    "\n",
    "# device = 'cuda'\n",
    "# sequence_len=1750 # sequence length of time series\n",
    "# max_len=5000 # max time series sequence length \n",
    "# n_head = 4 # number of attention head\n",
    "# n_layer = 2 # number of encoder layer\n",
    "# drop_prob = 0.1\n",
    "# d_model = 200 # number of dimension ( for positional embedding)\n",
    "# ffn_hidden = 512 # size of hidden layer before classification \n",
    "# in_features = 3 # for univariate time series (1d), it must be adjusted for 1. \n",
    "# n_classes = 2\n",
    "# net =  Transformer( in_features=in_features,\n",
    "#                      d_model=d_model,\n",
    "#                      details=False,\n",
    "#                      n_head=n_head,\n",
    "#                      max_len=max_len,\n",
    "#                      seq_len=sequence_len,\n",
    "#                      ffn_hidden=ffn_hidden,\n",
    "#                      n_layers=n_layer,\n",
    "#                      drop_prob=drop_prob,\n",
    "#                      n_classes=n_classes,\n",
    "#                      device=device\n",
    "#                      )\n",
    "# #summary(net, (2, 641),32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100000, Tr Loss: 0.8389, Tr Acc: 44.2857, Val Loss: 0.6958, Val Acc: 46.6667\n",
      "Epoch 101/100000, Tr Loss: 0.7147, Tr Acc: 52.8571, Val Loss: 0.6979, Val Acc: 43.3333\n",
      "Epoch 201/100000, Tr Loss: 0.7561, Tr Acc: 50.0000, Val Loss: 0.6975, Val Acc: 43.3333\n",
      "Epoch 301/100000, Tr Loss: 0.6991, Tr Acc: 57.1429, Val Loss: 0.6972, Val Acc: 40.0000\n",
      "Epoch 401/100000, Tr Loss: 0.7355, Tr Acc: 48.5714, Val Loss: 0.6968, Val Acc: 43.3333\n",
      "Epoch 501/100000, Tr Loss: 0.8307, Tr Acc: 44.2857, Val Loss: 0.6964, Val Acc: 43.3333\n",
      "Epoch 601/100000, Tr Loss: 0.7409, Tr Acc: 50.0000, Val Loss: 0.6963, Val Acc: 43.3333\n",
      "Epoch 701/100000, Tr Loss: 0.8305, Tr Acc: 45.7143, Val Loss: 0.6961, Val Acc: 43.3333\n",
      "Epoch 801/100000, Tr Loss: 0.7090, Tr Acc: 55.7143, Val Loss: 0.6961, Val Acc: 43.3333\n",
      "Epoch 901/100000, Tr Loss: 0.7110, Tr Acc: 52.8571, Val Loss: 0.6959, Val Acc: 43.3333\n",
      "Epoch 1001/100000, Tr Loss: 0.7124, Tr Acc: 57.1429, Val Loss: 0.6959, Val Acc: 43.3333\n",
      "Epoch 1101/100000, Tr Loss: 0.8021, Tr Acc: 44.2857, Val Loss: 0.6953, Val Acc: 46.6667\n",
      "Epoch 1201/100000, Tr Loss: 0.7584, Tr Acc: 47.1429, Val Loss: 0.6954, Val Acc: 46.6667\n",
      "Epoch 1301/100000, Tr Loss: 0.7803, Tr Acc: 45.7143, Val Loss: 0.6954, Val Acc: 50.0000\n",
      "Epoch 1401/100000, Tr Loss: 0.7090, Tr Acc: 48.5714, Val Loss: 0.6953, Val Acc: 56.6667\n",
      "Epoch 1501/100000, Tr Loss: 0.7635, Tr Acc: 51.4286, Val Loss: 0.6951, Val Acc: 56.6667\n",
      "Epoch 1601/100000, Tr Loss: 0.8138, Tr Acc: 41.4286, Val Loss: 0.6950, Val Acc: 56.6667\n",
      "Epoch 1701/100000, Tr Loss: 0.7966, Tr Acc: 44.2857, Val Loss: 0.6949, Val Acc: 53.3333\n",
      "Epoch 1801/100000, Tr Loss: 0.6578, Tr Acc: 57.1429, Val Loss: 0.6949, Val Acc: 53.3333\n",
      "Epoch 1901/100000, Tr Loss: 0.7754, Tr Acc: 47.1429, Val Loss: 0.6949, Val Acc: 53.3333\n",
      "Epoch 2001/100000, Tr Loss: 0.7265, Tr Acc: 47.1429, Val Loss: 0.6948, Val Acc: 53.3333\n",
      "Epoch 2101/100000, Tr Loss: 0.7203, Tr Acc: 58.5714, Val Loss: 0.6946, Val Acc: 53.3333\n",
      "Epoch 2201/100000, Tr Loss: 0.7366, Tr Acc: 51.4286, Val Loss: 0.6947, Val Acc: 53.3333\n",
      "Epoch 2301/100000, Tr Loss: 0.7662, Tr Acc: 57.1429, Val Loss: 0.6945, Val Acc: 56.6667\n",
      "Epoch 2401/100000, Tr Loss: 0.6860, Tr Acc: 52.8571, Val Loss: 0.6944, Val Acc: 53.3333\n",
      "Epoch 2501/100000, Tr Loss: 0.7633, Tr Acc: 51.4286, Val Loss: 0.6944, Val Acc: 56.6667\n",
      "Epoch 2601/100000, Tr Loss: 0.7455, Tr Acc: 52.8571, Val Loss: 0.6943, Val Acc: 56.6667\n",
      "Epoch 2701/100000, Tr Loss: 0.7030, Tr Acc: 52.8571, Val Loss: 0.6942, Val Acc: 56.6667\n",
      "Epoch 2801/100000, Tr Loss: 0.7343, Tr Acc: 48.5714, Val Loss: 0.6941, Val Acc: 56.6667\n",
      "Epoch 2901/100000, Tr Loss: 0.6792, Tr Acc: 52.8571, Val Loss: 0.6941, Val Acc: 53.3333\n",
      "Epoch 3001/100000, Tr Loss: 0.6765, Tr Acc: 61.4286, Val Loss: 0.6940, Val Acc: 60.0000\n",
      "Epoch 3101/100000, Tr Loss: 0.7275, Tr Acc: 51.4286, Val Loss: 0.6938, Val Acc: 56.6667\n",
      "Epoch 3201/100000, Tr Loss: 0.7605, Tr Acc: 48.5714, Val Loss: 0.6939, Val Acc: 60.0000\n",
      "Epoch 3301/100000, Tr Loss: 0.8181, Tr Acc: 48.5714, Val Loss: 0.6940, Val Acc: 56.6667\n",
      "Epoch 3401/100000, Tr Loss: 0.6873, Tr Acc: 61.4286, Val Loss: 0.6937, Val Acc: 56.6667\n",
      "Epoch 3501/100000, Tr Loss: 0.6866, Tr Acc: 60.0000, Val Loss: 0.6938, Val Acc: 56.6667\n",
      "Epoch 3601/100000, Tr Loss: 0.7780, Tr Acc: 50.0000, Val Loss: 0.6937, Val Acc: 56.6667\n",
      "Epoch 3701/100000, Tr Loss: 0.7509, Tr Acc: 50.0000, Val Loss: 0.6935, Val Acc: 56.6667\n",
      "Epoch 3801/100000, Tr Loss: 0.6756, Tr Acc: 54.2857, Val Loss: 0.6936, Val Acc: 56.6667\n",
      "Epoch 3901/100000, Tr Loss: 0.7054, Tr Acc: 64.2857, Val Loss: 0.6935, Val Acc: 56.6667\n",
      "Epoch 4001/100000, Tr Loss: 0.6775, Tr Acc: 60.0000, Val Loss: 0.6935, Val Acc: 56.6667\n",
      "Epoch 4101/100000, Tr Loss: 0.7762, Tr Acc: 41.4286, Val Loss: 0.6934, Val Acc: 56.6667\n",
      "Epoch 4201/100000, Tr Loss: 0.7024, Tr Acc: 50.0000, Val Loss: 0.6934, Val Acc: 56.6667\n",
      "Epoch 4301/100000, Tr Loss: 0.6563, Tr Acc: 62.8571, Val Loss: 0.6933, Val Acc: 56.6667\n",
      "Epoch 4401/100000, Tr Loss: 0.7251, Tr Acc: 57.1429, Val Loss: 0.6932, Val Acc: 56.6667\n",
      "Epoch 4501/100000, Tr Loss: 0.6711, Tr Acc: 60.0000, Val Loss: 0.6931, Val Acc: 56.6667\n",
      "Epoch 4601/100000, Tr Loss: 0.6673, Tr Acc: 55.7143, Val Loss: 0.6931, Val Acc: 56.6667\n",
      "Epoch 4701/100000, Tr Loss: 0.7902, Tr Acc: 50.0000, Val Loss: 0.6929, Val Acc: 56.6667\n",
      "Epoch 4801/100000, Tr Loss: 0.7760, Tr Acc: 52.8571, Val Loss: 0.6929, Val Acc: 56.6667\n",
      "Epoch 4901/100000, Tr Loss: 0.6346, Tr Acc: 62.8571, Val Loss: 0.6928, Val Acc: 56.6667\n",
      "Epoch 5001/100000, Tr Loss: 0.6965, Tr Acc: 61.4286, Val Loss: 0.6927, Val Acc: 56.6667\n",
      "Epoch 5101/100000, Tr Loss: 0.7844, Tr Acc: 42.8571, Val Loss: 0.6927, Val Acc: 56.6667\n",
      "Epoch 5201/100000, Tr Loss: 0.7260, Tr Acc: 54.2857, Val Loss: 0.6928, Val Acc: 60.0000\n",
      "Epoch 5301/100000, Tr Loss: 0.6650, Tr Acc: 57.1429, Val Loss: 0.6925, Val Acc: 63.3333\n",
      "Epoch 5401/100000, Tr Loss: 0.7802, Tr Acc: 52.8571, Val Loss: 0.6925, Val Acc: 63.3333\n",
      "Epoch 5501/100000, Tr Loss: 0.7109, Tr Acc: 48.5714, Val Loss: 0.6925, Val Acc: 63.3333\n",
      "Epoch 5601/100000, Tr Loss: 0.7106, Tr Acc: 57.1429, Val Loss: 0.6924, Val Acc: 60.0000\n",
      "Epoch 5701/100000, Tr Loss: 0.7292, Tr Acc: 57.1429, Val Loss: 0.6923, Val Acc: 63.3333\n",
      "Epoch 5801/100000, Tr Loss: 0.7394, Tr Acc: 61.4286, Val Loss: 0.6922, Val Acc: 63.3333\n",
      "Epoch 5901/100000, Tr Loss: 0.7538, Tr Acc: 50.0000, Val Loss: 0.6922, Val Acc: 60.0000\n",
      "Epoch 6001/100000, Tr Loss: 0.6790, Tr Acc: 60.0000, Val Loss: 0.6921, Val Acc: 60.0000\n",
      "Epoch 6101/100000, Tr Loss: 0.7256, Tr Acc: 52.8571, Val Loss: 0.6921, Val Acc: 60.0000\n",
      "Epoch 6201/100000, Tr Loss: 0.7326, Tr Acc: 52.8571, Val Loss: 0.6920, Val Acc: 60.0000\n",
      "Epoch 6301/100000, Tr Loss: 0.6681, Tr Acc: 60.0000, Val Loss: 0.6922, Val Acc: 60.0000\n",
      "Epoch 6401/100000, Tr Loss: 0.6761, Tr Acc: 52.8571, Val Loss: 0.6918, Val Acc: 60.0000\n",
      "Epoch 6501/100000, Tr Loss: 0.7182, Tr Acc: 55.7143, Val Loss: 0.6918, Val Acc: 60.0000\n",
      "Epoch 6601/100000, Tr Loss: 0.6976, Tr Acc: 54.2857, Val Loss: 0.6919, Val Acc: 60.0000\n",
      "Epoch 6701/100000, Tr Loss: 0.7094, Tr Acc: 47.1429, Val Loss: 0.6917, Val Acc: 60.0000\n",
      "Epoch 6801/100000, Tr Loss: 0.6658, Tr Acc: 62.8571, Val Loss: 0.6917, Val Acc: 60.0000\n",
      "Epoch 6901/100000, Tr Loss: 0.7429, Tr Acc: 50.0000, Val Loss: 0.6917, Val Acc: 60.0000\n",
      "Epoch 7001/100000, Tr Loss: 0.6894, Tr Acc: 47.1429, Val Loss: 0.6915, Val Acc: 60.0000\n",
      "Epoch 7101/100000, Tr Loss: 0.6762, Tr Acc: 54.2857, Val Loss: 0.6915, Val Acc: 60.0000\n",
      "Epoch 7201/100000, Tr Loss: 0.6946, Tr Acc: 62.8571, Val Loss: 0.6916, Val Acc: 60.0000\n",
      "Epoch 7301/100000, Tr Loss: 0.7189, Tr Acc: 58.5714, Val Loss: 0.6912, Val Acc: 60.0000\n",
      "Epoch 7401/100000, Tr Loss: 0.6484, Tr Acc: 62.8571, Val Loss: 0.6915, Val Acc: 60.0000\n",
      "Epoch 7501/100000, Tr Loss: 0.7291, Tr Acc: 50.0000, Val Loss: 0.6912, Val Acc: 60.0000\n",
      "Epoch 7601/100000, Tr Loss: 0.7001, Tr Acc: 54.2857, Val Loss: 0.6911, Val Acc: 60.0000\n",
      "Epoch 7701/100000, Tr Loss: 0.6336, Tr Acc: 70.0000, Val Loss: 0.6910, Val Acc: 60.0000\n",
      "Epoch 7801/100000, Tr Loss: 0.7496, Tr Acc: 52.8571, Val Loss: 0.6910, Val Acc: 60.0000\n",
      "Epoch 7901/100000, Tr Loss: 0.7474, Tr Acc: 45.7143, Val Loss: 0.6910, Val Acc: 60.0000\n",
      "Epoch 8001/100000, Tr Loss: 0.6698, Tr Acc: 55.7143, Val Loss: 0.6907, Val Acc: 60.0000\n",
      "Epoch 8101/100000, Tr Loss: 0.6907, Tr Acc: 52.8571, Val Loss: 0.6909, Val Acc: 60.0000\n",
      "Epoch 8201/100000, Tr Loss: 0.7344, Tr Acc: 57.1429, Val Loss: 0.6909, Val Acc: 60.0000\n",
      "Epoch 8301/100000, Tr Loss: 0.7504, Tr Acc: 47.1429, Val Loss: 0.6908, Val Acc: 60.0000\n",
      "Epoch 8401/100000, Tr Loss: 0.6929, Tr Acc: 60.0000, Val Loss: 0.6906, Val Acc: 60.0000\n",
      "Epoch 8501/100000, Tr Loss: 0.6876, Tr Acc: 55.7143, Val Loss: 0.6905, Val Acc: 60.0000\n",
      "Epoch 8601/100000, Tr Loss: 0.7608, Tr Acc: 54.2857, Val Loss: 0.6906, Val Acc: 60.0000\n",
      "Epoch 8701/100000, Tr Loss: 0.7640, Tr Acc: 40.0000, Val Loss: 0.6906, Val Acc: 60.0000\n",
      "Epoch 8801/100000, Tr Loss: 0.7673, Tr Acc: 55.7143, Val Loss: 0.6906, Val Acc: 60.0000\n",
      "Epoch 8901/100000, Tr Loss: 0.6884, Tr Acc: 51.4286, Val Loss: 0.6904, Val Acc: 60.0000\n",
      "Epoch 9001/100000, Tr Loss: 0.5985, Tr Acc: 74.2857, Val Loss: 0.6903, Val Acc: 60.0000\n",
      "Epoch 9101/100000, Tr Loss: 0.7523, Tr Acc: 51.4286, Val Loss: 0.6902, Val Acc: 60.0000\n",
      "Epoch 9201/100000, Tr Loss: 0.7171, Tr Acc: 48.5714, Val Loss: 0.6901, Val Acc: 60.0000\n",
      "Epoch 9301/100000, Tr Loss: 0.6801, Tr Acc: 61.4286, Val Loss: 0.6901, Val Acc: 60.0000\n",
      "Epoch 9401/100000, Tr Loss: 0.6687, Tr Acc: 57.1429, Val Loss: 0.6899, Val Acc: 60.0000\n",
      "Epoch 9501/100000, Tr Loss: 0.6306, Tr Acc: 65.7143, Val Loss: 0.6900, Val Acc: 60.0000\n",
      "Epoch 9601/100000, Tr Loss: 0.7102, Tr Acc: 58.5714, Val Loss: 0.6900, Val Acc: 60.0000\n",
      "Epoch 9701/100000, Tr Loss: 0.6867, Tr Acc: 58.5714, Val Loss: 0.6898, Val Acc: 60.0000\n",
      "Epoch 9801/100000, Tr Loss: 0.6673, Tr Acc: 57.1429, Val Loss: 0.6897, Val Acc: 60.0000\n",
      "Epoch 9901/100000, Tr Loss: 0.6863, Tr Acc: 55.7143, Val Loss: 0.6897, Val Acc: 60.0000\n",
      "Epoch 10001/100000, Tr Loss: 0.7211, Tr Acc: 52.8571, Val Loss: 0.6898, Val Acc: 60.0000\n",
      "Epoch 10101/100000, Tr Loss: 0.7094, Tr Acc: 58.5714, Val Loss: 0.6896, Val Acc: 60.0000\n",
      "Epoch 10201/100000, Tr Loss: 0.6950, Tr Acc: 54.2857, Val Loss: 0.6897, Val Acc: 60.0000\n",
      "Epoch 10301/100000, Tr Loss: 0.6901, Tr Acc: 58.5714, Val Loss: 0.6895, Val Acc: 60.0000\n",
      "Epoch 10401/100000, Tr Loss: 0.6909, Tr Acc: 61.4286, Val Loss: 0.6894, Val Acc: 60.0000\n",
      "Epoch 10501/100000, Tr Loss: 0.6932, Tr Acc: 58.5714, Val Loss: 0.6895, Val Acc: 60.0000\n",
      "Epoch 10601/100000, Tr Loss: 0.6801, Tr Acc: 55.7143, Val Loss: 0.6893, Val Acc: 60.0000\n",
      "Epoch 10701/100000, Tr Loss: 0.6504, Tr Acc: 65.7143, Val Loss: 0.6892, Val Acc: 60.0000\n",
      "Epoch 10801/100000, Tr Loss: 0.7699, Tr Acc: 52.8571, Val Loss: 0.6893, Val Acc: 60.0000\n",
      "Epoch 10901/100000, Tr Loss: 0.6712, Tr Acc: 57.1429, Val Loss: 0.6891, Val Acc: 60.0000\n",
      "Epoch 11001/100000, Tr Loss: 0.6717, Tr Acc: 60.0000, Val Loss: 0.6892, Val Acc: 60.0000\n",
      "Epoch 11101/100000, Tr Loss: 0.7413, Tr Acc: 51.4286, Val Loss: 0.6891, Val Acc: 60.0000\n",
      "Epoch 11201/100000, Tr Loss: 0.6850, Tr Acc: 58.5714, Val Loss: 0.6888, Val Acc: 60.0000\n",
      "Epoch 11301/100000, Tr Loss: 0.6733, Tr Acc: 57.1429, Val Loss: 0.6889, Val Acc: 60.0000\n",
      "Epoch 11401/100000, Tr Loss: 0.6853, Tr Acc: 58.5714, Val Loss: 0.6889, Val Acc: 60.0000\n",
      "Epoch 11501/100000, Tr Loss: 0.6835, Tr Acc: 62.8571, Val Loss: 0.6888, Val Acc: 63.3333\n",
      "Epoch 11601/100000, Tr Loss: 0.6106, Tr Acc: 68.5714, Val Loss: 0.6887, Val Acc: 63.3333\n",
      "Epoch 11701/100000, Tr Loss: 0.6814, Tr Acc: 57.1429, Val Loss: 0.6886, Val Acc: 63.3333\n",
      "Epoch 11801/100000, Tr Loss: 0.7370, Tr Acc: 50.0000, Val Loss: 0.6886, Val Acc: 63.3333\n",
      "Epoch 11901/100000, Tr Loss: 0.7456, Tr Acc: 51.4286, Val Loss: 0.6886, Val Acc: 63.3333\n",
      "Epoch 12001/100000, Tr Loss: 0.6890, Tr Acc: 61.4286, Val Loss: 0.6885, Val Acc: 63.3333\n",
      "Epoch 12101/100000, Tr Loss: 0.6287, Tr Acc: 70.0000, Val Loss: 0.6885, Val Acc: 63.3333\n",
      "Epoch 12201/100000, Tr Loss: 0.6006, Tr Acc: 70.0000, Val Loss: 0.6883, Val Acc: 63.3333\n",
      "Epoch 12301/100000, Tr Loss: 0.6906, Tr Acc: 57.1429, Val Loss: 0.6883, Val Acc: 63.3333\n",
      "Epoch 12401/100000, Tr Loss: 0.6130, Tr Acc: 62.8571, Val Loss: 0.6883, Val Acc: 63.3333\n",
      "Epoch 12501/100000, Tr Loss: 0.6235, Tr Acc: 64.2857, Val Loss: 0.6882, Val Acc: 63.3333\n",
      "Epoch 12601/100000, Tr Loss: 0.7214, Tr Acc: 51.4286, Val Loss: 0.6881, Val Acc: 63.3333\n",
      "Epoch 12701/100000, Tr Loss: 0.6885, Tr Acc: 58.5714, Val Loss: 0.6881, Val Acc: 60.0000\n",
      "Epoch 12801/100000, Tr Loss: 0.6718, Tr Acc: 57.1429, Val Loss: 0.6880, Val Acc: 60.0000\n",
      "Epoch 12901/100000, Tr Loss: 0.6868, Tr Acc: 57.1429, Val Loss: 0.6879, Val Acc: 60.0000\n",
      "Epoch 13001/100000, Tr Loss: 0.7195, Tr Acc: 54.2857, Val Loss: 0.6879, Val Acc: 60.0000\n",
      "Epoch 13101/100000, Tr Loss: 0.6958, Tr Acc: 54.2857, Val Loss: 0.6877, Val Acc: 60.0000\n",
      "Epoch 13201/100000, Tr Loss: 0.6444, Tr Acc: 60.0000, Val Loss: 0.6878, Val Acc: 56.6667\n",
      "Epoch 13301/100000, Tr Loss: 0.6476, Tr Acc: 52.8571, Val Loss: 0.6878, Val Acc: 56.6667\n",
      "Epoch 13401/100000, Tr Loss: 0.6725, Tr Acc: 61.4286, Val Loss: 0.6876, Val Acc: 56.6667\n",
      "Epoch 13501/100000, Tr Loss: 0.6178, Tr Acc: 68.5714, Val Loss: 0.6876, Val Acc: 60.0000\n",
      "Epoch 13601/100000, Tr Loss: 0.6885, Tr Acc: 52.8571, Val Loss: 0.6876, Val Acc: 60.0000\n",
      "Epoch 13701/100000, Tr Loss: 0.6818, Tr Acc: 57.1429, Val Loss: 0.6876, Val Acc: 56.6667\n",
      "Epoch 13801/100000, Tr Loss: 0.6325, Tr Acc: 58.5714, Val Loss: 0.6875, Val Acc: 56.6667\n",
      "Epoch 13901/100000, Tr Loss: 0.7218, Tr Acc: 52.8571, Val Loss: 0.6874, Val Acc: 56.6667\n",
      "Epoch 14001/100000, Tr Loss: 0.6866, Tr Acc: 54.2857, Val Loss: 0.6874, Val Acc: 56.6667\n",
      "Epoch 14101/100000, Tr Loss: 0.6313, Tr Acc: 70.0000, Val Loss: 0.6873, Val Acc: 56.6667\n",
      "Epoch 14201/100000, Tr Loss: 0.6987, Tr Acc: 55.7143, Val Loss: 0.6871, Val Acc: 56.6667\n",
      "Epoch 14301/100000, Tr Loss: 0.6683, Tr Acc: 57.1429, Val Loss: 0.6873, Val Acc: 56.6667\n",
      "Epoch 14401/100000, Tr Loss: 0.6267, Tr Acc: 61.4286, Val Loss: 0.6870, Val Acc: 56.6667\n",
      "Epoch 14501/100000, Tr Loss: 0.7192, Tr Acc: 57.1429, Val Loss: 0.6871, Val Acc: 56.6667\n",
      "Epoch 14601/100000, Tr Loss: 0.6815, Tr Acc: 57.1429, Val Loss: 0.6871, Val Acc: 56.6667\n",
      "Epoch 14701/100000, Tr Loss: 0.6628, Tr Acc: 57.1429, Val Loss: 0.6869, Val Acc: 56.6667\n",
      "Epoch 14801/100000, Tr Loss: 0.7187, Tr Acc: 52.8571, Val Loss: 0.6868, Val Acc: 56.6667\n",
      "Epoch 14901/100000, Tr Loss: 0.6636, Tr Acc: 55.7143, Val Loss: 0.6867, Val Acc: 56.6667\n",
      "Epoch 15001/100000, Tr Loss: 0.5938, Tr Acc: 70.0000, Val Loss: 0.6866, Val Acc: 56.6667\n",
      "Epoch 15101/100000, Tr Loss: 0.6258, Tr Acc: 65.7143, Val Loss: 0.6866, Val Acc: 56.6667\n",
      "Epoch 15201/100000, Tr Loss: 0.6569, Tr Acc: 62.8571, Val Loss: 0.6866, Val Acc: 56.6667\n",
      "Epoch 15301/100000, Tr Loss: 0.6258, Tr Acc: 62.8571, Val Loss: 0.6865, Val Acc: 56.6667\n",
      "Epoch 15401/100000, Tr Loss: 0.6898, Tr Acc: 61.4286, Val Loss: 0.6863, Val Acc: 56.6667\n",
      "Epoch 15501/100000, Tr Loss: 0.7201, Tr Acc: 64.2857, Val Loss: 0.6863, Val Acc: 56.6667\n",
      "Epoch 15601/100000, Tr Loss: 0.6014, Tr Acc: 68.5714, Val Loss: 0.6866, Val Acc: 56.6667\n",
      "Epoch 15701/100000, Tr Loss: 0.6777, Tr Acc: 58.5714, Val Loss: 0.6863, Val Acc: 56.6667\n",
      "Epoch 15801/100000, Tr Loss: 0.6993, Tr Acc: 55.7143, Val Loss: 0.6863, Val Acc: 56.6667\n",
      "Epoch 15901/100000, Tr Loss: 0.6000, Tr Acc: 65.7143, Val Loss: 0.6861, Val Acc: 56.6667\n",
      "Epoch 16001/100000, Tr Loss: 0.6510, Tr Acc: 54.2857, Val Loss: 0.6861, Val Acc: 56.6667\n",
      "Epoch 16101/100000, Tr Loss: 0.7359, Tr Acc: 51.4286, Val Loss: 0.6861, Val Acc: 56.6667\n",
      "Epoch 16201/100000, Tr Loss: 0.6592, Tr Acc: 62.8571, Val Loss: 0.6860, Val Acc: 56.6667\n",
      "Epoch 16301/100000, Tr Loss: 0.7190, Tr Acc: 45.7143, Val Loss: 0.6858, Val Acc: 56.6667\n",
      "Epoch 16401/100000, Tr Loss: 0.7043, Tr Acc: 57.1429, Val Loss: 0.6858, Val Acc: 56.6667\n",
      "Epoch 16501/100000, Tr Loss: 0.6874, Tr Acc: 52.8571, Val Loss: 0.6857, Val Acc: 56.6667\n",
      "Epoch 16601/100000, Tr Loss: 0.6642, Tr Acc: 55.7143, Val Loss: 0.6858, Val Acc: 56.6667\n",
      "Epoch 16701/100000, Tr Loss: 0.6046, Tr Acc: 64.2857, Val Loss: 0.6858, Val Acc: 56.6667\n",
      "Epoch 16801/100000, Tr Loss: 0.5652, Tr Acc: 74.2857, Val Loss: 0.6857, Val Acc: 56.6667\n",
      "Epoch 16901/100000, Tr Loss: 0.6067, Tr Acc: 67.1429, Val Loss: 0.6855, Val Acc: 56.6667\n",
      "Epoch 17001/100000, Tr Loss: 0.5653, Tr Acc: 74.2857, Val Loss: 0.6857, Val Acc: 56.6667\n",
      "Epoch 17101/100000, Tr Loss: 0.6231, Tr Acc: 62.8571, Val Loss: 0.6857, Val Acc: 56.6667\n",
      "Epoch 17201/100000, Tr Loss: 0.7002, Tr Acc: 61.4286, Val Loss: 0.6857, Val Acc: 56.6667\n",
      "Epoch 17301/100000, Tr Loss: 0.6883, Tr Acc: 52.8571, Val Loss: 0.6853, Val Acc: 56.6667\n",
      "Epoch 17401/100000, Tr Loss: 0.6771, Tr Acc: 57.1429, Val Loss: 0.6853, Val Acc: 56.6667\n",
      "Epoch 17501/100000, Tr Loss: 0.6357, Tr Acc: 65.7143, Val Loss: 0.6851, Val Acc: 56.6667\n",
      "Epoch 17601/100000, Tr Loss: 0.6274, Tr Acc: 68.5714, Val Loss: 0.6852, Val Acc: 56.6667\n",
      "Epoch 17701/100000, Tr Loss: 0.6978, Tr Acc: 52.8571, Val Loss: 0.6852, Val Acc: 56.6667\n",
      "Epoch 17801/100000, Tr Loss: 0.7560, Tr Acc: 52.8571, Val Loss: 0.6851, Val Acc: 56.6667\n",
      "Epoch 17901/100000, Tr Loss: 0.6056, Tr Acc: 72.8571, Val Loss: 0.6852, Val Acc: 56.6667\n",
      "Epoch 18001/100000, Tr Loss: 0.6974, Tr Acc: 61.4286, Val Loss: 0.6850, Val Acc: 56.6667\n",
      "Epoch 18101/100000, Tr Loss: 0.6615, Tr Acc: 62.8571, Val Loss: 0.6850, Val Acc: 56.6667\n",
      "Epoch 18201/100000, Tr Loss: 0.6318, Tr Acc: 58.5714, Val Loss: 0.6850, Val Acc: 56.6667\n",
      "Epoch 18301/100000, Tr Loss: 0.6942, Tr Acc: 54.2857, Val Loss: 0.6849, Val Acc: 56.6667\n",
      "Epoch 18401/100000, Tr Loss: 0.5922, Tr Acc: 64.2857, Val Loss: 0.6847, Val Acc: 56.6667\n",
      "Epoch 18501/100000, Tr Loss: 0.6542, Tr Acc: 68.5714, Val Loss: 0.6848, Val Acc: 56.6667\n",
      "Epoch 18601/100000, Tr Loss: 0.6154, Tr Acc: 65.7143, Val Loss: 0.6846, Val Acc: 56.6667\n",
      "Epoch 18701/100000, Tr Loss: 0.5945, Tr Acc: 65.7143, Val Loss: 0.6846, Val Acc: 56.6667\n",
      "Epoch 18801/100000, Tr Loss: 0.6783, Tr Acc: 57.1429, Val Loss: 0.6846, Val Acc: 56.6667\n",
      "Epoch 18901/100000, Tr Loss: 0.6881, Tr Acc: 57.1429, Val Loss: 0.6844, Val Acc: 56.6667\n",
      "Epoch 19001/100000, Tr Loss: 0.6556, Tr Acc: 55.7143, Val Loss: 0.6845, Val Acc: 56.6667\n",
      "Epoch 19101/100000, Tr Loss: 0.6712, Tr Acc: 57.1429, Val Loss: 0.6844, Val Acc: 56.6667\n",
      "Epoch 19201/100000, Tr Loss: 0.6263, Tr Acc: 64.2857, Val Loss: 0.6843, Val Acc: 56.6667\n",
      "Epoch 19301/100000, Tr Loss: 0.5928, Tr Acc: 68.5714, Val Loss: 0.6843, Val Acc: 56.6667\n",
      "Epoch 19401/100000, Tr Loss: 0.6743, Tr Acc: 60.0000, Val Loss: 0.6842, Val Acc: 56.6667\n",
      "Epoch 19501/100000, Tr Loss: 0.6155, Tr Acc: 67.1429, Val Loss: 0.6840, Val Acc: 56.6667\n",
      "Epoch 19601/100000, Tr Loss: 0.6219, Tr Acc: 68.5714, Val Loss: 0.6840, Val Acc: 56.6667\n",
      "Epoch 19701/100000, Tr Loss: 0.6025, Tr Acc: 67.1429, Val Loss: 0.6842, Val Acc: 56.6667\n",
      "Epoch 19801/100000, Tr Loss: 0.6397, Tr Acc: 62.8571, Val Loss: 0.6839, Val Acc: 56.6667\n",
      "Epoch 19901/100000, Tr Loss: 0.6700, Tr Acc: 62.8571, Val Loss: 0.6839, Val Acc: 56.6667\n",
      "Epoch 20001/100000, Tr Loss: 0.6716, Tr Acc: 54.2857, Val Loss: 0.6837, Val Acc: 56.6667\n",
      "Epoch 20101/100000, Tr Loss: 0.6660, Tr Acc: 52.8571, Val Loss: 0.6838, Val Acc: 56.6667\n",
      "Epoch 20201/100000, Tr Loss: 0.6772, Tr Acc: 54.2857, Val Loss: 0.6838, Val Acc: 56.6667\n",
      "Epoch 20301/100000, Tr Loss: 0.6351, Tr Acc: 58.5714, Val Loss: 0.6836, Val Acc: 56.6667\n",
      "Epoch 20401/100000, Tr Loss: 0.6388, Tr Acc: 55.7143, Val Loss: 0.6837, Val Acc: 56.6667\n",
      "Epoch 20501/100000, Tr Loss: 0.6685, Tr Acc: 57.1429, Val Loss: 0.6836, Val Acc: 56.6667\n",
      "Epoch 20601/100000, Tr Loss: 0.6389, Tr Acc: 61.4286, Val Loss: 0.6835, Val Acc: 56.6667\n",
      "Epoch 20701/100000, Tr Loss: 0.6319, Tr Acc: 62.8571, Val Loss: 0.6835, Val Acc: 56.6667\n",
      "Epoch 20801/100000, Tr Loss: 0.5638, Tr Acc: 67.1429, Val Loss: 0.6835, Val Acc: 56.6667\n",
      "Epoch 20901/100000, Tr Loss: 0.6166, Tr Acc: 68.5714, Val Loss: 0.6834, Val Acc: 56.6667\n",
      "Epoch 21001/100000, Tr Loss: 0.6619, Tr Acc: 62.8571, Val Loss: 0.6833, Val Acc: 56.6667\n",
      "Epoch 21101/100000, Tr Loss: 0.6046, Tr Acc: 67.1429, Val Loss: 0.6832, Val Acc: 56.6667\n",
      "Epoch 21201/100000, Tr Loss: 0.6778, Tr Acc: 62.8571, Val Loss: 0.6831, Val Acc: 56.6667\n",
      "Epoch 21301/100000, Tr Loss: 0.5957, Tr Acc: 74.2857, Val Loss: 0.6832, Val Acc: 56.6667\n",
      "Epoch 21401/100000, Tr Loss: 0.6714, Tr Acc: 62.8571, Val Loss: 0.6830, Val Acc: 56.6667\n",
      "Epoch 21501/100000, Tr Loss: 0.6351, Tr Acc: 62.8571, Val Loss: 0.6830, Val Acc: 56.6667\n",
      "Epoch 21601/100000, Tr Loss: 0.7163, Tr Acc: 57.1429, Val Loss: 0.6831, Val Acc: 56.6667\n",
      "Epoch 21701/100000, Tr Loss: 0.6801, Tr Acc: 60.0000, Val Loss: 0.6831, Val Acc: 56.6667\n",
      "Epoch 21801/100000, Tr Loss: 0.6362, Tr Acc: 58.5714, Val Loss: 0.6829, Val Acc: 56.6667\n",
      "Epoch 21901/100000, Tr Loss: 0.6106, Tr Acc: 67.1429, Val Loss: 0.6828, Val Acc: 56.6667\n",
      "Epoch 22001/100000, Tr Loss: 0.6291, Tr Acc: 61.4286, Val Loss: 0.6830, Val Acc: 56.6667\n",
      "Epoch 22101/100000, Tr Loss: 0.6179, Tr Acc: 64.2857, Val Loss: 0.6829, Val Acc: 60.0000\n",
      "Epoch 22201/100000, Tr Loss: 0.6270, Tr Acc: 61.4286, Val Loss: 0.6828, Val Acc: 60.0000\n",
      "Epoch 22301/100000, Tr Loss: 0.6144, Tr Acc: 62.8571, Val Loss: 0.6827, Val Acc: 60.0000\n",
      "Epoch 22401/100000, Tr Loss: 0.6316, Tr Acc: 67.1429, Val Loss: 0.6826, Val Acc: 60.0000\n",
      "Epoch 22501/100000, Tr Loss: 0.7591, Tr Acc: 48.5714, Val Loss: 0.6826, Val Acc: 60.0000\n",
      "Epoch 22601/100000, Tr Loss: 0.6791, Tr Acc: 54.2857, Val Loss: 0.6826, Val Acc: 60.0000\n",
      "Epoch 22701/100000, Tr Loss: 0.6700, Tr Acc: 54.2857, Val Loss: 0.6825, Val Acc: 60.0000\n",
      "Epoch 22801/100000, Tr Loss: 0.6065, Tr Acc: 62.8571, Val Loss: 0.6825, Val Acc: 60.0000\n",
      "Epoch 22901/100000, Tr Loss: 0.5986, Tr Acc: 68.5714, Val Loss: 0.6823, Val Acc: 60.0000\n",
      "Epoch 23001/100000, Tr Loss: 0.6202, Tr Acc: 60.0000, Val Loss: 0.6823, Val Acc: 60.0000\n",
      "Epoch 23101/100000, Tr Loss: 0.6369, Tr Acc: 70.0000, Val Loss: 0.6823, Val Acc: 60.0000\n",
      "Epoch 23201/100000, Tr Loss: 0.6172, Tr Acc: 62.8571, Val Loss: 0.6821, Val Acc: 60.0000\n",
      "Epoch 23301/100000, Tr Loss: 0.6299, Tr Acc: 57.1429, Val Loss: 0.6821, Val Acc: 60.0000\n",
      "Epoch 23401/100000, Tr Loss: 0.6765, Tr Acc: 57.1429, Val Loss: 0.6823, Val Acc: 60.0000\n",
      "Epoch 23501/100000, Tr Loss: 0.5720, Tr Acc: 70.0000, Val Loss: 0.6821, Val Acc: 60.0000\n",
      "Epoch 23601/100000, Tr Loss: 0.6279, Tr Acc: 62.8571, Val Loss: 0.6822, Val Acc: 60.0000\n",
      "Epoch 23701/100000, Tr Loss: 0.6331, Tr Acc: 64.2857, Val Loss: 0.6821, Val Acc: 60.0000\n",
      "Epoch 23801/100000, Tr Loss: 0.6170, Tr Acc: 62.8571, Val Loss: 0.6819, Val Acc: 60.0000\n",
      "Epoch 23901/100000, Tr Loss: 0.6229, Tr Acc: 62.8571, Val Loss: 0.6819, Val Acc: 60.0000\n",
      "Epoch 24001/100000, Tr Loss: 0.6853, Tr Acc: 60.0000, Val Loss: 0.6819, Val Acc: 60.0000\n",
      "Epoch 24101/100000, Tr Loss: 0.6541, Tr Acc: 58.5714, Val Loss: 0.6820, Val Acc: 60.0000\n",
      "Epoch 24201/100000, Tr Loss: 0.6325, Tr Acc: 58.5714, Val Loss: 0.6818, Val Acc: 60.0000\n",
      "Epoch 24301/100000, Tr Loss: 0.6653, Tr Acc: 60.0000, Val Loss: 0.6816, Val Acc: 60.0000\n",
      "Epoch 24401/100000, Tr Loss: 0.7069, Tr Acc: 58.5714, Val Loss: 0.6816, Val Acc: 60.0000\n",
      "Epoch 24501/100000, Tr Loss: 0.5666, Tr Acc: 70.0000, Val Loss: 0.6816, Val Acc: 60.0000\n",
      "Epoch 24601/100000, Tr Loss: 0.7155, Tr Acc: 55.7143, Val Loss: 0.6816, Val Acc: 60.0000\n",
      "Epoch 24701/100000, Tr Loss: 0.5838, Tr Acc: 68.5714, Val Loss: 0.6816, Val Acc: 60.0000\n",
      "Epoch 24801/100000, Tr Loss: 0.6529, Tr Acc: 62.8571, Val Loss: 0.6814, Val Acc: 60.0000\n",
      "Epoch 24901/100000, Tr Loss: 0.5975, Tr Acc: 70.0000, Val Loss: 0.6815, Val Acc: 60.0000\n",
      "Epoch 25001/100000, Tr Loss: 0.6757, Tr Acc: 47.1429, Val Loss: 0.6812, Val Acc: 60.0000\n",
      "Epoch 25101/100000, Tr Loss: 0.6418, Tr Acc: 61.4286, Val Loss: 0.6813, Val Acc: 60.0000\n",
      "Epoch 25201/100000, Tr Loss: 0.6291, Tr Acc: 60.0000, Val Loss: 0.6813, Val Acc: 60.0000\n",
      "Epoch 25301/100000, Tr Loss: 0.6498, Tr Acc: 68.5714, Val Loss: 0.6813, Val Acc: 60.0000\n",
      "Epoch 25401/100000, Tr Loss: 0.5905, Tr Acc: 65.7143, Val Loss: 0.6813, Val Acc: 63.3333\n",
      "Epoch 25501/100000, Tr Loss: 0.5839, Tr Acc: 62.8571, Val Loss: 0.6812, Val Acc: 60.0000\n",
      "Epoch 25601/100000, Tr Loss: 0.6344, Tr Acc: 64.2857, Val Loss: 0.6810, Val Acc: 63.3333\n",
      "Epoch 25701/100000, Tr Loss: 0.6429, Tr Acc: 65.7143, Val Loss: 0.6809, Val Acc: 63.3333\n",
      "Epoch 25801/100000, Tr Loss: 0.5735, Tr Acc: 75.7143, Val Loss: 0.6810, Val Acc: 63.3333\n",
      "Epoch 25901/100000, Tr Loss: 0.5771, Tr Acc: 68.5714, Val Loss: 0.6810, Val Acc: 63.3333\n",
      "Epoch 26001/100000, Tr Loss: 0.6176, Tr Acc: 65.7143, Val Loss: 0.6810, Val Acc: 63.3333\n",
      "Epoch 26101/100000, Tr Loss: 0.5605, Tr Acc: 74.2857, Val Loss: 0.6810, Val Acc: 63.3333\n",
      "Epoch 26201/100000, Tr Loss: 0.6171, Tr Acc: 68.5714, Val Loss: 0.6809, Val Acc: 63.3333\n",
      "Epoch 26301/100000, Tr Loss: 0.6224, Tr Acc: 62.8571, Val Loss: 0.6808, Val Acc: 66.6667\n",
      "Epoch 26401/100000, Tr Loss: 0.5794, Tr Acc: 74.2857, Val Loss: 0.6807, Val Acc: 63.3333\n",
      "Epoch 26501/100000, Tr Loss: 0.6707, Tr Acc: 51.4286, Val Loss: 0.6807, Val Acc: 63.3333\n",
      "Epoch 26601/100000, Tr Loss: 0.6230, Tr Acc: 64.2857, Val Loss: 0.6806, Val Acc: 63.3333\n",
      "Epoch 26701/100000, Tr Loss: 0.6137, Tr Acc: 72.8571, Val Loss: 0.6807, Val Acc: 66.6667\n",
      "Epoch 26801/100000, Tr Loss: 0.6283, Tr Acc: 61.4286, Val Loss: 0.6805, Val Acc: 66.6667\n",
      "Epoch 26901/100000, Tr Loss: 0.6589, Tr Acc: 68.5714, Val Loss: 0.6807, Val Acc: 66.6667\n",
      "Epoch 27001/100000, Tr Loss: 0.6022, Tr Acc: 67.1429, Val Loss: 0.6804, Val Acc: 66.6667\n",
      "Epoch 27101/100000, Tr Loss: 0.6624, Tr Acc: 61.4286, Val Loss: 0.6805, Val Acc: 66.6667\n",
      "Epoch 27201/100000, Tr Loss: 0.6045, Tr Acc: 68.5714, Val Loss: 0.6803, Val Acc: 66.6667\n",
      "Epoch 27301/100000, Tr Loss: 0.5922, Tr Acc: 68.5714, Val Loss: 0.6803, Val Acc: 66.6667\n",
      "Epoch 27401/100000, Tr Loss: 0.6747, Tr Acc: 58.5714, Val Loss: 0.6803, Val Acc: 66.6667\n",
      "Epoch 27501/100000, Tr Loss: 0.5982, Tr Acc: 68.5714, Val Loss: 0.6803, Val Acc: 66.6667\n",
      "Epoch 27601/100000, Tr Loss: 0.6070, Tr Acc: 64.2857, Val Loss: 0.6801, Val Acc: 66.6667\n",
      "Epoch 27701/100000, Tr Loss: 0.6414, Tr Acc: 60.0000, Val Loss: 0.6802, Val Acc: 66.6667\n",
      "Epoch 27801/100000, Tr Loss: 0.6161, Tr Acc: 68.5714, Val Loss: 0.6803, Val Acc: 66.6667\n",
      "Epoch 27901/100000, Tr Loss: 0.6120, Tr Acc: 67.1429, Val Loss: 0.6802, Val Acc: 66.6667\n",
      "Epoch 28001/100000, Tr Loss: 0.6212, Tr Acc: 62.8571, Val Loss: 0.6800, Val Acc: 66.6667\n",
      "Epoch 28101/100000, Tr Loss: 0.6224, Tr Acc: 68.5714, Val Loss: 0.6799, Val Acc: 66.6667\n",
      "Epoch 28201/100000, Tr Loss: 0.5974, Tr Acc: 71.4286, Val Loss: 0.6800, Val Acc: 66.6667\n",
      "Epoch 28301/100000, Tr Loss: 0.5837, Tr Acc: 72.8571, Val Loss: 0.6797, Val Acc: 70.0000\n",
      "Epoch 28401/100000, Tr Loss: 0.6153, Tr Acc: 71.4286, Val Loss: 0.6799, Val Acc: 70.0000\n",
      "Epoch 28501/100000, Tr Loss: 0.6356, Tr Acc: 62.8571, Val Loss: 0.6797, Val Acc: 70.0000\n",
      "Epoch 28601/100000, Tr Loss: 0.5947, Tr Acc: 74.2857, Val Loss: 0.6798, Val Acc: 70.0000\n",
      "Epoch 28701/100000, Tr Loss: 0.6005, Tr Acc: 67.1429, Val Loss: 0.6797, Val Acc: 70.0000\n",
      "Epoch 28801/100000, Tr Loss: 0.6018, Tr Acc: 67.1429, Val Loss: 0.6796, Val Acc: 70.0000\n",
      "Epoch 28901/100000, Tr Loss: 0.6160, Tr Acc: 65.7143, Val Loss: 0.6796, Val Acc: 70.0000\n",
      "Epoch 29001/100000, Tr Loss: 0.6007, Tr Acc: 68.5714, Val Loss: 0.6796, Val Acc: 70.0000\n",
      "Epoch 29101/100000, Tr Loss: 0.6298, Tr Acc: 61.4286, Val Loss: 0.6795, Val Acc: 70.0000\n",
      "Epoch 29201/100000, Tr Loss: 0.6039, Tr Acc: 67.1429, Val Loss: 0.6797, Val Acc: 70.0000\n",
      "Epoch 29301/100000, Tr Loss: 0.5999, Tr Acc: 67.1429, Val Loss: 0.6793, Val Acc: 70.0000\n",
      "Epoch 29401/100000, Tr Loss: 0.6088, Tr Acc: 62.8571, Val Loss: 0.6793, Val Acc: 70.0000\n",
      "Epoch 29501/100000, Tr Loss: 0.6468, Tr Acc: 58.5714, Val Loss: 0.6793, Val Acc: 70.0000\n",
      "Epoch 29601/100000, Tr Loss: 0.5501, Tr Acc: 75.7143, Val Loss: 0.6794, Val Acc: 70.0000\n",
      "Epoch 29701/100000, Tr Loss: 0.6129, Tr Acc: 70.0000, Val Loss: 0.6791, Val Acc: 70.0000\n",
      "Epoch 29801/100000, Tr Loss: 0.5935, Tr Acc: 62.8571, Val Loss: 0.6792, Val Acc: 70.0000\n",
      "Epoch 29901/100000, Tr Loss: 0.6509, Tr Acc: 62.8571, Val Loss: 0.6791, Val Acc: 70.0000\n",
      "Epoch 30001/100000, Tr Loss: 0.6544, Tr Acc: 58.5714, Val Loss: 0.6792, Val Acc: 70.0000\n",
      "Epoch 30101/100000, Tr Loss: 0.5983, Tr Acc: 64.2857, Val Loss: 0.6792, Val Acc: 70.0000\n",
      "Epoch 30201/100000, Tr Loss: 0.5722, Tr Acc: 67.1429, Val Loss: 0.6790, Val Acc: 70.0000\n",
      "Epoch 30301/100000, Tr Loss: 0.6304, Tr Acc: 61.4286, Val Loss: 0.6790, Val Acc: 70.0000\n",
      "Epoch 30401/100000, Tr Loss: 0.5749, Tr Acc: 70.0000, Val Loss: 0.6789, Val Acc: 70.0000\n",
      "Epoch 30501/100000, Tr Loss: 0.5924, Tr Acc: 64.2857, Val Loss: 0.6790, Val Acc: 70.0000\n",
      "Epoch 30601/100000, Tr Loss: 0.5758, Tr Acc: 65.7143, Val Loss: 0.6790, Val Acc: 70.0000\n",
      "Epoch 30701/100000, Tr Loss: 0.5719, Tr Acc: 68.5714, Val Loss: 0.6788, Val Acc: 70.0000\n",
      "Epoch 30801/100000, Tr Loss: 0.5900, Tr Acc: 65.7143, Val Loss: 0.6786, Val Acc: 70.0000\n",
      "Epoch 30901/100000, Tr Loss: 0.5988, Tr Acc: 65.7143, Val Loss: 0.6787, Val Acc: 70.0000\n",
      "Epoch 31001/100000, Tr Loss: 0.5907, Tr Acc: 70.0000, Val Loss: 0.6787, Val Acc: 70.0000\n",
      "Epoch 31101/100000, Tr Loss: 0.5628, Tr Acc: 70.0000, Val Loss: 0.6787, Val Acc: 70.0000\n",
      "Epoch 31201/100000, Tr Loss: 0.6051, Tr Acc: 64.2857, Val Loss: 0.6787, Val Acc: 70.0000\n",
      "Epoch 31301/100000, Tr Loss: 0.5646, Tr Acc: 68.5714, Val Loss: 0.6785, Val Acc: 70.0000\n",
      "Epoch 31401/100000, Tr Loss: 0.6173, Tr Acc: 62.8571, Val Loss: 0.6785, Val Acc: 70.0000\n",
      "Epoch 31501/100000, Tr Loss: 0.6649, Tr Acc: 60.0000, Val Loss: 0.6783, Val Acc: 70.0000\n",
      "Epoch 31601/100000, Tr Loss: 0.5531, Tr Acc: 72.8571, Val Loss: 0.6784, Val Acc: 70.0000\n",
      "Epoch 31701/100000, Tr Loss: 0.6585, Tr Acc: 61.4286, Val Loss: 0.6782, Val Acc: 70.0000\n",
      "Epoch 31801/100000, Tr Loss: 0.5032, Tr Acc: 81.4286, Val Loss: 0.6782, Val Acc: 70.0000\n",
      "Epoch 31901/100000, Tr Loss: 0.5690, Tr Acc: 72.8571, Val Loss: 0.6783, Val Acc: 70.0000\n",
      "Epoch 32001/100000, Tr Loss: 0.6247, Tr Acc: 70.0000, Val Loss: 0.6782, Val Acc: 70.0000\n",
      "Epoch 32101/100000, Tr Loss: 0.5697, Tr Acc: 67.1429, Val Loss: 0.6780, Val Acc: 70.0000\n",
      "Epoch 32201/100000, Tr Loss: 0.6422, Tr Acc: 62.8571, Val Loss: 0.6780, Val Acc: 70.0000\n",
      "Epoch 32301/100000, Tr Loss: 0.5726, Tr Acc: 72.8571, Val Loss: 0.6781, Val Acc: 70.0000\n",
      "Epoch 32401/100000, Tr Loss: 0.5828, Tr Acc: 67.1429, Val Loss: 0.6780, Val Acc: 70.0000\n",
      "Epoch 32501/100000, Tr Loss: 0.6529, Tr Acc: 64.2857, Val Loss: 0.6780, Val Acc: 70.0000\n",
      "Epoch 32601/100000, Tr Loss: 0.6182, Tr Acc: 70.0000, Val Loss: 0.6780, Val Acc: 70.0000\n",
      "Epoch 32701/100000, Tr Loss: 0.5370, Tr Acc: 72.8571, Val Loss: 0.6779, Val Acc: 70.0000\n",
      "Epoch 32801/100000, Tr Loss: 0.5762, Tr Acc: 70.0000, Val Loss: 0.6778, Val Acc: 70.0000\n",
      "Epoch 32901/100000, Tr Loss: 0.6779, Tr Acc: 57.1429, Val Loss: 0.6777, Val Acc: 70.0000\n",
      "Epoch 33001/100000, Tr Loss: 0.5568, Tr Acc: 71.4286, Val Loss: 0.6778, Val Acc: 70.0000\n",
      "Epoch 33101/100000, Tr Loss: 0.5507, Tr Acc: 68.5714, Val Loss: 0.6776, Val Acc: 70.0000\n",
      "Epoch 33201/100000, Tr Loss: 0.6133, Tr Acc: 67.1429, Val Loss: 0.6777, Val Acc: 70.0000\n",
      "Epoch 33301/100000, Tr Loss: 0.5966, Tr Acc: 62.8571, Val Loss: 0.6777, Val Acc: 70.0000\n",
      "Epoch 33401/100000, Tr Loss: 0.6250, Tr Acc: 60.0000, Val Loss: 0.6777, Val Acc: 70.0000\n",
      "Epoch 33501/100000, Tr Loss: 0.5922, Tr Acc: 70.0000, Val Loss: 0.6777, Val Acc: 70.0000\n",
      "Epoch 33601/100000, Tr Loss: 0.6012, Tr Acc: 62.8571, Val Loss: 0.6774, Val Acc: 70.0000\n",
      "Epoch 33701/100000, Tr Loss: 0.5693, Tr Acc: 70.0000, Val Loss: 0.6774, Val Acc: 70.0000\n",
      "Epoch 33801/100000, Tr Loss: 0.5340, Tr Acc: 67.1429, Val Loss: 0.6774, Val Acc: 70.0000\n",
      "Epoch 33901/100000, Tr Loss: 0.6337, Tr Acc: 67.1429, Val Loss: 0.6774, Val Acc: 70.0000\n",
      "Epoch 34001/100000, Tr Loss: 0.6176, Tr Acc: 65.7143, Val Loss: 0.6773, Val Acc: 70.0000\n",
      "Epoch 34101/100000, Tr Loss: 0.5825, Tr Acc: 65.7143, Val Loss: 0.6773, Val Acc: 70.0000\n",
      "Epoch 34201/100000, Tr Loss: 0.5637, Tr Acc: 65.7143, Val Loss: 0.6773, Val Acc: 70.0000\n",
      "Epoch 34301/100000, Tr Loss: 0.6366, Tr Acc: 68.5714, Val Loss: 0.6772, Val Acc: 70.0000\n",
      "Epoch 34401/100000, Tr Loss: 0.5034, Tr Acc: 78.5714, Val Loss: 0.6773, Val Acc: 70.0000\n",
      "Epoch 34501/100000, Tr Loss: 0.5730, Tr Acc: 70.0000, Val Loss: 0.6771, Val Acc: 70.0000\n",
      "Epoch 34601/100000, Tr Loss: 0.6062, Tr Acc: 70.0000, Val Loss: 0.6771, Val Acc: 70.0000\n",
      "Epoch 34701/100000, Tr Loss: 0.5665, Tr Acc: 71.4286, Val Loss: 0.6770, Val Acc: 70.0000\n",
      "Epoch 34801/100000, Tr Loss: 0.5645, Tr Acc: 71.4286, Val Loss: 0.6770, Val Acc: 70.0000\n",
      "Epoch 34901/100000, Tr Loss: 0.5651, Tr Acc: 67.1429, Val Loss: 0.6771, Val Acc: 70.0000\n",
      "Epoch 35001/100000, Tr Loss: 0.5986, Tr Acc: 65.7143, Val Loss: 0.6769, Val Acc: 70.0000\n",
      "Epoch 35101/100000, Tr Loss: 0.5897, Tr Acc: 71.4286, Val Loss: 0.6770, Val Acc: 70.0000\n",
      "Epoch 35201/100000, Tr Loss: 0.5559, Tr Acc: 74.2857, Val Loss: 0.6768, Val Acc: 70.0000\n",
      "Epoch 35301/100000, Tr Loss: 0.5635, Tr Acc: 71.4286, Val Loss: 0.6767, Val Acc: 70.0000\n",
      "Epoch 35401/100000, Tr Loss: 0.5753, Tr Acc: 70.0000, Val Loss: 0.6768, Val Acc: 70.0000\n",
      "Epoch 35501/100000, Tr Loss: 0.5917, Tr Acc: 70.0000, Val Loss: 0.6767, Val Acc: 70.0000\n",
      "Epoch 35601/100000, Tr Loss: 0.5539, Tr Acc: 68.5714, Val Loss: 0.6766, Val Acc: 70.0000\n",
      "Epoch 35701/100000, Tr Loss: 0.5475, Tr Acc: 68.5714, Val Loss: 0.6766, Val Acc: 70.0000\n",
      "Epoch 35801/100000, Tr Loss: 0.5573, Tr Acc: 71.4286, Val Loss: 0.6767, Val Acc: 70.0000\n",
      "Epoch 35901/100000, Tr Loss: 0.5396, Tr Acc: 78.5714, Val Loss: 0.6764, Val Acc: 70.0000\n",
      "Epoch 36001/100000, Tr Loss: 0.5499, Tr Acc: 75.7143, Val Loss: 0.6764, Val Acc: 70.0000\n",
      "Epoch 36101/100000, Tr Loss: 0.5445, Tr Acc: 75.7143, Val Loss: 0.6763, Val Acc: 70.0000\n",
      "Epoch 36201/100000, Tr Loss: 0.5008, Tr Acc: 81.4286, Val Loss: 0.6765, Val Acc: 70.0000\n",
      "Epoch 36301/100000, Tr Loss: 0.5448, Tr Acc: 72.8571, Val Loss: 0.6764, Val Acc: 70.0000\n",
      "Epoch 36401/100000, Tr Loss: 0.6009, Tr Acc: 74.2857, Val Loss: 0.6764, Val Acc: 70.0000\n",
      "Epoch 36501/100000, Tr Loss: 0.5537, Tr Acc: 70.0000, Val Loss: 0.6764, Val Acc: 70.0000\n",
      "Epoch 36601/100000, Tr Loss: 0.5774, Tr Acc: 70.0000, Val Loss: 0.6762, Val Acc: 70.0000\n",
      "Epoch 36701/100000, Tr Loss: 0.5174, Tr Acc: 78.5714, Val Loss: 0.6760, Val Acc: 70.0000\n",
      "Epoch 36801/100000, Tr Loss: 0.5703, Tr Acc: 70.0000, Val Loss: 0.6760, Val Acc: 70.0000\n",
      "Epoch 36901/100000, Tr Loss: 0.5831, Tr Acc: 67.1429, Val Loss: 0.6761, Val Acc: 70.0000\n",
      "Epoch 37001/100000, Tr Loss: 0.5004, Tr Acc: 81.4286, Val Loss: 0.6761, Val Acc: 70.0000\n",
      "Epoch 37101/100000, Tr Loss: 0.5356, Tr Acc: 72.8571, Val Loss: 0.6760, Val Acc: 70.0000\n",
      "Epoch 37201/100000, Tr Loss: 0.5357, Tr Acc: 72.8571, Val Loss: 0.6760, Val Acc: 70.0000\n",
      "Epoch 37301/100000, Tr Loss: 0.5478, Tr Acc: 77.1429, Val Loss: 0.6759, Val Acc: 70.0000\n",
      "Epoch 37401/100000, Tr Loss: 0.5716, Tr Acc: 70.0000, Val Loss: 0.6760, Val Acc: 70.0000\n",
      "Epoch 37501/100000, Tr Loss: 0.5672, Tr Acc: 72.8571, Val Loss: 0.6759, Val Acc: 70.0000\n",
      "Epoch 37601/100000, Tr Loss: 0.5829, Tr Acc: 67.1429, Val Loss: 0.6758, Val Acc: 70.0000\n",
      "Epoch 37701/100000, Tr Loss: 0.6131, Tr Acc: 68.5714, Val Loss: 0.6759, Val Acc: 70.0000\n",
      "Epoch 37801/100000, Tr Loss: 0.5426, Tr Acc: 71.4286, Val Loss: 0.6757, Val Acc: 70.0000\n",
      "Epoch 37901/100000, Tr Loss: 0.5783, Tr Acc: 64.2857, Val Loss: 0.6757, Val Acc: 70.0000\n",
      "Epoch 38001/100000, Tr Loss: 0.5556, Tr Acc: 72.8571, Val Loss: 0.6757, Val Acc: 70.0000\n",
      "Epoch 38101/100000, Tr Loss: 0.5680, Tr Acc: 70.0000, Val Loss: 0.6755, Val Acc: 70.0000\n",
      "Epoch 38201/100000, Tr Loss: 0.6130, Tr Acc: 62.8571, Val Loss: 0.6756, Val Acc: 70.0000\n",
      "Epoch 38301/100000, Tr Loss: 0.5401, Tr Acc: 80.0000, Val Loss: 0.6756, Val Acc: 70.0000\n",
      "Epoch 38401/100000, Tr Loss: 0.6161, Tr Acc: 64.2857, Val Loss: 0.6754, Val Acc: 70.0000\n",
      "Epoch 38501/100000, Tr Loss: 0.5793, Tr Acc: 65.7143, Val Loss: 0.6753, Val Acc: 70.0000\n",
      "Epoch 38601/100000, Tr Loss: 0.5098, Tr Acc: 77.1429, Val Loss: 0.6755, Val Acc: 70.0000\n",
      "Epoch 38701/100000, Tr Loss: 0.5768, Tr Acc: 70.0000, Val Loss: 0.6752, Val Acc: 70.0000\n",
      "Epoch 38801/100000, Tr Loss: 0.5439, Tr Acc: 77.1429, Val Loss: 0.6752, Val Acc: 70.0000\n",
      "Epoch 38901/100000, Tr Loss: 0.6164, Tr Acc: 67.1429, Val Loss: 0.6753, Val Acc: 70.0000\n",
      "Epoch 39001/100000, Tr Loss: 0.5868, Tr Acc: 60.0000, Val Loss: 0.6751, Val Acc: 70.0000\n",
      "Epoch 39101/100000, Tr Loss: 0.5446, Tr Acc: 81.4286, Val Loss: 0.6750, Val Acc: 70.0000\n",
      "Epoch 39201/100000, Tr Loss: 0.5467, Tr Acc: 77.1429, Val Loss: 0.6751, Val Acc: 70.0000\n",
      "Epoch 39301/100000, Tr Loss: 0.5787, Tr Acc: 70.0000, Val Loss: 0.6751, Val Acc: 70.0000\n",
      "Epoch 39401/100000, Tr Loss: 0.5786, Tr Acc: 65.7143, Val Loss: 0.6752, Val Acc: 70.0000\n",
      "Epoch 39501/100000, Tr Loss: 0.5885, Tr Acc: 70.0000, Val Loss: 0.6752, Val Acc: 70.0000\n",
      "Epoch 39601/100000, Tr Loss: 0.5848, Tr Acc: 70.0000, Val Loss: 0.6750, Val Acc: 70.0000\n",
      "Epoch 39701/100000, Tr Loss: 0.5011, Tr Acc: 75.7143, Val Loss: 0.6752, Val Acc: 70.0000\n",
      "Epoch 39801/100000, Tr Loss: 0.5693, Tr Acc: 68.5714, Val Loss: 0.6749, Val Acc: 70.0000\n",
      "Epoch 39901/100000, Tr Loss: 0.5320, Tr Acc: 75.7143, Val Loss: 0.6750, Val Acc: 70.0000\n",
      "Epoch 40001/100000, Tr Loss: 0.5550, Tr Acc: 72.8571, Val Loss: 0.6746, Val Acc: 70.0000\n",
      "Epoch 40101/100000, Tr Loss: 0.5211, Tr Acc: 74.2857, Val Loss: 0.6749, Val Acc: 70.0000\n",
      "Epoch 40201/100000, Tr Loss: 0.5813, Tr Acc: 67.1429, Val Loss: 0.6747, Val Acc: 70.0000\n",
      "Epoch 40301/100000, Tr Loss: 0.6142, Tr Acc: 62.8571, Val Loss: 0.6748, Val Acc: 70.0000\n",
      "Epoch 40401/100000, Tr Loss: 0.5811, Tr Acc: 74.2857, Val Loss: 0.6748, Val Acc: 70.0000\n",
      "Epoch 40501/100000, Tr Loss: 0.5834, Tr Acc: 71.4286, Val Loss: 0.6746, Val Acc: 70.0000\n",
      "Epoch 40601/100000, Tr Loss: 0.6531, Tr Acc: 57.1429, Val Loss: 0.6746, Val Acc: 70.0000\n",
      "Epoch 40701/100000, Tr Loss: 0.5194, Tr Acc: 71.4286, Val Loss: 0.6745, Val Acc: 70.0000\n",
      "Epoch 40801/100000, Tr Loss: 0.6019, Tr Acc: 60.0000, Val Loss: 0.6744, Val Acc: 70.0000\n",
      "Epoch 40901/100000, Tr Loss: 0.5544, Tr Acc: 70.0000, Val Loss: 0.6744, Val Acc: 70.0000\n",
      "Epoch 41001/100000, Tr Loss: 0.5587, Tr Acc: 67.1429, Val Loss: 0.6746, Val Acc: 70.0000\n",
      "Epoch 41101/100000, Tr Loss: 0.6211, Tr Acc: 72.8571, Val Loss: 0.6745, Val Acc: 70.0000\n",
      "Epoch 41201/100000, Tr Loss: 0.5654, Tr Acc: 62.8571, Val Loss: 0.6743, Val Acc: 70.0000\n",
      "Epoch 41301/100000, Tr Loss: 0.5466, Tr Acc: 75.7143, Val Loss: 0.6744, Val Acc: 70.0000\n",
      "Epoch 41401/100000, Tr Loss: 0.5285, Tr Acc: 75.7143, Val Loss: 0.6741, Val Acc: 70.0000\n",
      "Epoch 41501/100000, Tr Loss: 0.5512, Tr Acc: 71.4286, Val Loss: 0.6742, Val Acc: 70.0000\n",
      "Epoch 41601/100000, Tr Loss: 0.5132, Tr Acc: 81.4286, Val Loss: 0.6741, Val Acc: 70.0000\n",
      "Epoch 41701/100000, Tr Loss: 0.5884, Tr Acc: 72.8571, Val Loss: 0.6742, Val Acc: 70.0000\n",
      "Epoch 41801/100000, Tr Loss: 0.4810, Tr Acc: 80.0000, Val Loss: 0.6742, Val Acc: 70.0000\n",
      "Epoch 41901/100000, Tr Loss: 0.5611, Tr Acc: 70.0000, Val Loss: 0.6743, Val Acc: 70.0000\n",
      "Epoch 42001/100000, Tr Loss: 0.5173, Tr Acc: 77.1429, Val Loss: 0.6740, Val Acc: 70.0000\n",
      "Epoch 42101/100000, Tr Loss: 0.5197, Tr Acc: 74.2857, Val Loss: 0.6741, Val Acc: 70.0000\n",
      "Epoch 42201/100000, Tr Loss: 0.5621, Tr Acc: 71.4286, Val Loss: 0.6740, Val Acc: 70.0000\n",
      "Epoch 42301/100000, Tr Loss: 0.5965, Tr Acc: 68.5714, Val Loss: 0.6741, Val Acc: 70.0000\n",
      "Epoch 42401/100000, Tr Loss: 0.5663, Tr Acc: 71.4286, Val Loss: 0.6738, Val Acc: 70.0000\n",
      "Epoch 42501/100000, Tr Loss: 0.5174, Tr Acc: 77.1429, Val Loss: 0.6738, Val Acc: 70.0000\n",
      "Epoch 42601/100000, Tr Loss: 0.5723, Tr Acc: 72.8571, Val Loss: 0.6737, Val Acc: 70.0000\n",
      "Epoch 42701/100000, Tr Loss: 0.5954, Tr Acc: 65.7143, Val Loss: 0.6738, Val Acc: 70.0000\n",
      "Epoch 42801/100000, Tr Loss: 0.5417, Tr Acc: 75.7143, Val Loss: 0.6737, Val Acc: 70.0000\n",
      "Epoch 42901/100000, Tr Loss: 0.5753, Tr Acc: 71.4286, Val Loss: 0.6736, Val Acc: 70.0000\n",
      "Epoch 43001/100000, Tr Loss: 0.6316, Tr Acc: 64.2857, Val Loss: 0.6736, Val Acc: 70.0000\n",
      "Epoch 43101/100000, Tr Loss: 0.5147, Tr Acc: 81.4286, Val Loss: 0.6736, Val Acc: 70.0000\n",
      "Epoch 43201/100000, Tr Loss: 0.5673, Tr Acc: 70.0000, Val Loss: 0.6735, Val Acc: 70.0000\n",
      "Epoch 43301/100000, Tr Loss: 0.5264, Tr Acc: 77.1429, Val Loss: 0.6734, Val Acc: 70.0000\n",
      "Epoch 43401/100000, Tr Loss: 0.5552, Tr Acc: 71.4286, Val Loss: 0.6735, Val Acc: 70.0000\n",
      "Epoch 43501/100000, Tr Loss: 0.5142, Tr Acc: 77.1429, Val Loss: 0.6735, Val Acc: 70.0000\n",
      "Epoch 43601/100000, Tr Loss: 0.5229, Tr Acc: 74.2857, Val Loss: 0.6734, Val Acc: 70.0000\n",
      "Epoch 43701/100000, Tr Loss: 0.6002, Tr Acc: 64.2857, Val Loss: 0.6732, Val Acc: 70.0000\n",
      "Epoch 43801/100000, Tr Loss: 0.5159, Tr Acc: 78.5714, Val Loss: 0.6734, Val Acc: 70.0000\n",
      "Epoch 43901/100000, Tr Loss: 0.5654, Tr Acc: 67.1429, Val Loss: 0.6731, Val Acc: 70.0000\n",
      "Epoch 44001/100000, Tr Loss: 0.5167, Tr Acc: 72.8571, Val Loss: 0.6734, Val Acc: 70.0000\n",
      "Epoch 44101/100000, Tr Loss: 0.5628, Tr Acc: 68.5714, Val Loss: 0.6732, Val Acc: 70.0000\n",
      "Epoch 44201/100000, Tr Loss: 0.5258, Tr Acc: 71.4286, Val Loss: 0.6732, Val Acc: 70.0000\n",
      "Epoch 44301/100000, Tr Loss: 0.6104, Tr Acc: 65.7143, Val Loss: 0.6731, Val Acc: 70.0000\n",
      "Epoch 44401/100000, Tr Loss: 0.5889, Tr Acc: 67.1429, Val Loss: 0.6731, Val Acc: 66.6667\n",
      "Epoch 44501/100000, Tr Loss: 0.5533, Tr Acc: 74.2857, Val Loss: 0.6730, Val Acc: 66.6667\n",
      "Epoch 44601/100000, Tr Loss: 0.5593, Tr Acc: 67.1429, Val Loss: 0.6730, Val Acc: 66.6667\n",
      "Epoch 44701/100000, Tr Loss: 0.5307, Tr Acc: 72.8571, Val Loss: 0.6728, Val Acc: 70.0000\n",
      "Epoch 44801/100000, Tr Loss: 0.5222, Tr Acc: 67.1429, Val Loss: 0.6730, Val Acc: 70.0000\n",
      "Epoch 44901/100000, Tr Loss: 0.4997, Tr Acc: 80.0000, Val Loss: 0.6727, Val Acc: 70.0000\n",
      "Epoch 45001/100000, Tr Loss: 0.4896, Tr Acc: 81.4286, Val Loss: 0.6728, Val Acc: 66.6667\n",
      "Epoch 45101/100000, Tr Loss: 0.5211, Tr Acc: 80.0000, Val Loss: 0.6728, Val Acc: 66.6667\n",
      "Epoch 45201/100000, Tr Loss: 0.5672, Tr Acc: 67.1429, Val Loss: 0.6726, Val Acc: 66.6667\n",
      "Epoch 45301/100000, Tr Loss: 0.5830, Tr Acc: 61.4286, Val Loss: 0.6728, Val Acc: 63.3333\n",
      "Epoch 45401/100000, Tr Loss: 0.5431, Tr Acc: 75.7143, Val Loss: 0.6727, Val Acc: 63.3333\n",
      "Epoch 45501/100000, Tr Loss: 0.5672, Tr Acc: 75.7143, Val Loss: 0.6727, Val Acc: 63.3333\n",
      "Epoch 45601/100000, Tr Loss: 0.5393, Tr Acc: 74.2857, Val Loss: 0.6725, Val Acc: 66.6667\n",
      "Epoch 45701/100000, Tr Loss: 0.5179, Tr Acc: 77.1429, Val Loss: 0.6726, Val Acc: 66.6667\n",
      "Epoch 45801/100000, Tr Loss: 0.5308, Tr Acc: 72.8571, Val Loss: 0.6725, Val Acc: 66.6667\n",
      "Epoch 45901/100000, Tr Loss: 0.5168, Tr Acc: 72.8571, Val Loss: 0.6726, Val Acc: 66.6667\n",
      "Epoch 46001/100000, Tr Loss: 0.5395, Tr Acc: 70.0000, Val Loss: 0.6725, Val Acc: 66.6667\n",
      "Epoch 46101/100000, Tr Loss: 0.5247, Tr Acc: 74.2857, Val Loss: 0.6722, Val Acc: 66.6667\n",
      "Epoch 46201/100000, Tr Loss: 0.5315, Tr Acc: 74.2857, Val Loss: 0.6723, Val Acc: 66.6667\n",
      "Epoch 46301/100000, Tr Loss: 0.5413, Tr Acc: 70.0000, Val Loss: 0.6723, Val Acc: 66.6667\n",
      "Epoch 46401/100000, Tr Loss: 0.4867, Tr Acc: 78.5714, Val Loss: 0.6723, Val Acc: 63.3333\n",
      "Epoch 46501/100000, Tr Loss: 0.5077, Tr Acc: 78.5714, Val Loss: 0.6723, Val Acc: 63.3333\n",
      "Epoch 46601/100000, Tr Loss: 0.4704, Tr Acc: 78.5714, Val Loss: 0.6723, Val Acc: 63.3333\n",
      "Epoch 46701/100000, Tr Loss: 0.4975, Tr Acc: 81.4286, Val Loss: 0.6723, Val Acc: 63.3333\n",
      "Epoch 46801/100000, Tr Loss: 0.5410, Tr Acc: 70.0000, Val Loss: 0.6720, Val Acc: 63.3333\n",
      "Epoch 46901/100000, Tr Loss: 0.5033, Tr Acc: 81.4286, Val Loss: 0.6721, Val Acc: 63.3333\n",
      "Epoch 47001/100000, Tr Loss: 0.5016, Tr Acc: 75.7143, Val Loss: 0.6719, Val Acc: 66.6667\n",
      "Epoch 47101/100000, Tr Loss: 0.5690, Tr Acc: 71.4286, Val Loss: 0.6721, Val Acc: 63.3333\n",
      "Epoch 47201/100000, Tr Loss: 0.5698, Tr Acc: 65.7143, Val Loss: 0.6721, Val Acc: 63.3333\n",
      "Epoch 47301/100000, Tr Loss: 0.5753, Tr Acc: 72.8571, Val Loss: 0.6720, Val Acc: 63.3333\n",
      "Epoch 47401/100000, Tr Loss: 0.5490, Tr Acc: 74.2857, Val Loss: 0.6720, Val Acc: 63.3333\n",
      "Epoch 47501/100000, Tr Loss: 0.5193, Tr Acc: 80.0000, Val Loss: 0.6719, Val Acc: 63.3333\n",
      "Epoch 47601/100000, Tr Loss: 0.5395, Tr Acc: 68.5714, Val Loss: 0.6719, Val Acc: 63.3333\n",
      "Epoch 47701/100000, Tr Loss: 0.5188, Tr Acc: 74.2857, Val Loss: 0.6716, Val Acc: 63.3333\n",
      "Epoch 47801/100000, Tr Loss: 0.5697, Tr Acc: 68.5714, Val Loss: 0.6719, Val Acc: 63.3333\n",
      "Epoch 47901/100000, Tr Loss: 0.4939, Tr Acc: 75.7143, Val Loss: 0.6717, Val Acc: 63.3333\n",
      "Epoch 48001/100000, Tr Loss: 0.5166, Tr Acc: 78.5714, Val Loss: 0.6718, Val Acc: 63.3333\n",
      "Epoch 48101/100000, Tr Loss: 0.4979, Tr Acc: 81.4286, Val Loss: 0.6715, Val Acc: 63.3333\n",
      "Epoch 48201/100000, Tr Loss: 0.5246, Tr Acc: 75.7143, Val Loss: 0.6716, Val Acc: 63.3333\n",
      "Epoch 48301/100000, Tr Loss: 0.5371, Tr Acc: 75.7143, Val Loss: 0.6715, Val Acc: 63.3333\n",
      "Epoch 48401/100000, Tr Loss: 0.5829, Tr Acc: 70.0000, Val Loss: 0.6715, Val Acc: 63.3333\n",
      "Epoch 48501/100000, Tr Loss: 0.4612, Tr Acc: 80.0000, Val Loss: 0.6715, Val Acc: 63.3333\n",
      "Epoch 48601/100000, Tr Loss: 0.5920, Tr Acc: 75.7143, Val Loss: 0.6714, Val Acc: 63.3333\n",
      "Epoch 48701/100000, Tr Loss: 0.5324, Tr Acc: 74.2857, Val Loss: 0.6714, Val Acc: 63.3333\n",
      "Epoch 48801/100000, Tr Loss: 0.5644, Tr Acc: 74.2857, Val Loss: 0.6714, Val Acc: 63.3333\n",
      "Epoch 48901/100000, Tr Loss: 0.5173, Tr Acc: 78.5714, Val Loss: 0.6711, Val Acc: 63.3333\n",
      "Epoch 49001/100000, Tr Loss: 0.5074, Tr Acc: 78.5714, Val Loss: 0.6713, Val Acc: 63.3333\n",
      "Epoch 49101/100000, Tr Loss: 0.5447, Tr Acc: 72.8571, Val Loss: 0.6712, Val Acc: 63.3333\n",
      "Epoch 49201/100000, Tr Loss: 0.4941, Tr Acc: 77.1429, Val Loss: 0.6712, Val Acc: 63.3333\n",
      "Epoch 49301/100000, Tr Loss: 0.5781, Tr Acc: 74.2857, Val Loss: 0.6712, Val Acc: 63.3333\n",
      "Epoch 49401/100000, Tr Loss: 0.4809, Tr Acc: 82.8571, Val Loss: 0.6710, Val Acc: 63.3333\n",
      "Epoch 49501/100000, Tr Loss: 0.5055, Tr Acc: 74.2857, Val Loss: 0.6710, Val Acc: 63.3333\n",
      "Epoch 49601/100000, Tr Loss: 0.5034, Tr Acc: 80.0000, Val Loss: 0.6710, Val Acc: 63.3333\n",
      "Epoch 49701/100000, Tr Loss: 0.6002, Tr Acc: 68.5714, Val Loss: 0.6711, Val Acc: 63.3333\n",
      "Epoch 49801/100000, Tr Loss: 0.5542, Tr Acc: 80.0000, Val Loss: 0.6710, Val Acc: 63.3333\n",
      "Epoch 49901/100000, Tr Loss: 0.5009, Tr Acc: 72.8571, Val Loss: 0.6709, Val Acc: 63.3333\n",
      "Epoch 50001/100000, Tr Loss: 0.5167, Tr Acc: 72.8571, Val Loss: 0.6710, Val Acc: 63.3333\n",
      "Epoch 50101/100000, Tr Loss: 0.4921, Tr Acc: 77.1429, Val Loss: 0.6709, Val Acc: 63.3333\n",
      "Epoch 50201/100000, Tr Loss: 0.5009, Tr Acc: 71.4286, Val Loss: 0.6708, Val Acc: 63.3333\n",
      "Epoch 50301/100000, Tr Loss: 0.5238, Tr Acc: 74.2857, Val Loss: 0.6707, Val Acc: 63.3333\n",
      "Epoch 50401/100000, Tr Loss: 0.4833, Tr Acc: 82.8571, Val Loss: 0.6708, Val Acc: 63.3333\n",
      "Epoch 50501/100000, Tr Loss: 0.4644, Tr Acc: 78.5714, Val Loss: 0.6705, Val Acc: 63.3333\n",
      "Epoch 50601/100000, Tr Loss: 0.5234, Tr Acc: 68.5714, Val Loss: 0.6706, Val Acc: 63.3333\n",
      "Epoch 50701/100000, Tr Loss: 0.4974, Tr Acc: 82.8571, Val Loss: 0.6706, Val Acc: 63.3333\n",
      "Epoch 50801/100000, Tr Loss: 0.5451, Tr Acc: 72.8571, Val Loss: 0.6704, Val Acc: 63.3333\n",
      "Epoch 50901/100000, Tr Loss: 0.5192, Tr Acc: 78.5714, Val Loss: 0.6706, Val Acc: 63.3333\n",
      "Epoch 51001/100000, Tr Loss: 0.5361, Tr Acc: 71.4286, Val Loss: 0.6704, Val Acc: 63.3333\n",
      "Epoch 51101/100000, Tr Loss: 0.5341, Tr Acc: 65.7143, Val Loss: 0.6704, Val Acc: 63.3333\n",
      "Epoch 51201/100000, Tr Loss: 0.5149, Tr Acc: 78.5714, Val Loss: 0.6705, Val Acc: 63.3333\n",
      "Epoch 51301/100000, Tr Loss: 0.4982, Tr Acc: 74.2857, Val Loss: 0.6704, Val Acc: 63.3333\n",
      "Epoch 51401/100000, Tr Loss: 0.5165, Tr Acc: 74.2857, Val Loss: 0.6702, Val Acc: 63.3333\n",
      "Epoch 51501/100000, Tr Loss: 0.6011, Tr Acc: 65.7143, Val Loss: 0.6704, Val Acc: 63.3333\n",
      "Epoch 51601/100000, Tr Loss: 0.5212, Tr Acc: 78.5714, Val Loss: 0.6701, Val Acc: 63.3333\n",
      "Epoch 51701/100000, Tr Loss: 0.5532, Tr Acc: 71.4286, Val Loss: 0.6702, Val Acc: 63.3333\n",
      "Epoch 51801/100000, Tr Loss: 0.4568, Tr Acc: 85.7143, Val Loss: 0.6701, Val Acc: 63.3333\n",
      "Epoch 51901/100000, Tr Loss: 0.5481, Tr Acc: 68.5714, Val Loss: 0.6703, Val Acc: 63.3333\n",
      "Epoch 52001/100000, Tr Loss: 0.5298, Tr Acc: 71.4286, Val Loss: 0.6701, Val Acc: 63.3333\n",
      "Epoch 52101/100000, Tr Loss: 0.5121, Tr Acc: 77.1429, Val Loss: 0.6699, Val Acc: 63.3333\n",
      "Epoch 52201/100000, Tr Loss: 0.5263, Tr Acc: 75.7143, Val Loss: 0.6700, Val Acc: 63.3333\n",
      "Epoch 52301/100000, Tr Loss: 0.5081, Tr Acc: 82.8571, Val Loss: 0.6699, Val Acc: 63.3333\n",
      "Epoch 52401/100000, Tr Loss: 0.5094, Tr Acc: 74.2857, Val Loss: 0.6699, Val Acc: 63.3333\n",
      "Epoch 52501/100000, Tr Loss: 0.5432, Tr Acc: 72.8571, Val Loss: 0.6699, Val Acc: 63.3333\n",
      "Epoch 52601/100000, Tr Loss: 0.4922, Tr Acc: 77.1429, Val Loss: 0.6700, Val Acc: 63.3333\n",
      "Epoch 52701/100000, Tr Loss: 0.4696, Tr Acc: 80.0000, Val Loss: 0.6697, Val Acc: 63.3333\n",
      "Epoch 52801/100000, Tr Loss: 0.5336, Tr Acc: 78.5714, Val Loss: 0.6697, Val Acc: 63.3333\n",
      "Epoch 52901/100000, Tr Loss: 0.5027, Tr Acc: 77.1429, Val Loss: 0.6697, Val Acc: 63.3333\n",
      "Epoch 53001/100000, Tr Loss: 0.5451, Tr Acc: 65.7143, Val Loss: 0.6696, Val Acc: 63.3333\n",
      "Epoch 53101/100000, Tr Loss: 0.4566, Tr Acc: 82.8571, Val Loss: 0.6697, Val Acc: 63.3333\n",
      "Epoch 53201/100000, Tr Loss: 0.5181, Tr Acc: 80.0000, Val Loss: 0.6696, Val Acc: 63.3333\n",
      "Epoch 53301/100000, Tr Loss: 0.5186, Tr Acc: 75.7143, Val Loss: 0.6696, Val Acc: 63.3333\n",
      "Epoch 53401/100000, Tr Loss: 0.5698, Tr Acc: 71.4286, Val Loss: 0.6695, Val Acc: 63.3333\n",
      "Epoch 53501/100000, Tr Loss: 0.4454, Tr Acc: 82.8571, Val Loss: 0.6695, Val Acc: 63.3333\n",
      "Epoch 53601/100000, Tr Loss: 0.5011, Tr Acc: 74.2857, Val Loss: 0.6694, Val Acc: 63.3333\n",
      "Epoch 53701/100000, Tr Loss: 0.5100, Tr Acc: 75.7143, Val Loss: 0.6695, Val Acc: 63.3333\n",
      "Epoch 53801/100000, Tr Loss: 0.4851, Tr Acc: 77.1429, Val Loss: 0.6694, Val Acc: 63.3333\n",
      "Epoch 53901/100000, Tr Loss: 0.4713, Tr Acc: 78.5714, Val Loss: 0.6693, Val Acc: 63.3333\n",
      "Epoch 54001/100000, Tr Loss: 0.5066, Tr Acc: 80.0000, Val Loss: 0.6693, Val Acc: 63.3333\n",
      "Epoch 54101/100000, Tr Loss: 0.5190, Tr Acc: 75.7143, Val Loss: 0.6693, Val Acc: 63.3333\n",
      "Epoch 54201/100000, Tr Loss: 0.5414, Tr Acc: 67.1429, Val Loss: 0.6693, Val Acc: 63.3333\n",
      "Epoch 54301/100000, Tr Loss: 0.5749, Tr Acc: 67.1429, Val Loss: 0.6692, Val Acc: 63.3333\n",
      "Epoch 54401/100000, Tr Loss: 0.5028, Tr Acc: 80.0000, Val Loss: 0.6694, Val Acc: 63.3333\n",
      "Epoch 54501/100000, Tr Loss: 0.5442, Tr Acc: 70.0000, Val Loss: 0.6689, Val Acc: 63.3333\n",
      "Epoch 54601/100000, Tr Loss: 0.4985, Tr Acc: 78.5714, Val Loss: 0.6689, Val Acc: 63.3333\n",
      "Epoch 54701/100000, Tr Loss: 0.5109, Tr Acc: 77.1429, Val Loss: 0.6691, Val Acc: 63.3333\n",
      "Epoch 54801/100000, Tr Loss: 0.4525, Tr Acc: 80.0000, Val Loss: 0.6691, Val Acc: 63.3333\n",
      "Epoch 54901/100000, Tr Loss: 0.4970, Tr Acc: 70.0000, Val Loss: 0.6688, Val Acc: 63.3333\n",
      "Epoch 55001/100000, Tr Loss: 0.4878, Tr Acc: 75.7143, Val Loss: 0.6687, Val Acc: 63.3333\n",
      "Epoch 55101/100000, Tr Loss: 0.5597, Tr Acc: 74.2857, Val Loss: 0.6689, Val Acc: 63.3333\n",
      "Epoch 55201/100000, Tr Loss: 0.5134, Tr Acc: 77.1429, Val Loss: 0.6688, Val Acc: 63.3333\n",
      "Epoch 55301/100000, Tr Loss: 0.4533, Tr Acc: 87.1429, Val Loss: 0.6687, Val Acc: 63.3333\n",
      "Epoch 55401/100000, Tr Loss: 0.5117, Tr Acc: 75.7143, Val Loss: 0.6688, Val Acc: 63.3333\n",
      "Epoch 55501/100000, Tr Loss: 0.4841, Tr Acc: 85.7143, Val Loss: 0.6687, Val Acc: 63.3333\n",
      "Epoch 55601/100000, Tr Loss: 0.5197, Tr Acc: 77.1429, Val Loss: 0.6686, Val Acc: 63.3333\n",
      "Epoch 55701/100000, Tr Loss: 0.5415, Tr Acc: 75.7143, Val Loss: 0.6686, Val Acc: 63.3333\n",
      "Epoch 55801/100000, Tr Loss: 0.4794, Tr Acc: 74.2857, Val Loss: 0.6684, Val Acc: 63.3333\n",
      "Epoch 55901/100000, Tr Loss: 0.5181, Tr Acc: 75.7143, Val Loss: 0.6685, Val Acc: 63.3333\n",
      "Epoch 56001/100000, Tr Loss: 0.4949, Tr Acc: 71.4286, Val Loss: 0.6686, Val Acc: 63.3333\n",
      "Epoch 56101/100000, Tr Loss: 0.5156, Tr Acc: 74.2857, Val Loss: 0.6686, Val Acc: 63.3333\n",
      "Epoch 56201/100000, Tr Loss: 0.4911, Tr Acc: 74.2857, Val Loss: 0.6683, Val Acc: 63.3333\n",
      "Epoch 56301/100000, Tr Loss: 0.5013, Tr Acc: 77.1429, Val Loss: 0.6684, Val Acc: 63.3333\n",
      "Epoch 56401/100000, Tr Loss: 0.4821, Tr Acc: 82.8571, Val Loss: 0.6682, Val Acc: 63.3333\n",
      "Epoch 56501/100000, Tr Loss: 0.4831, Tr Acc: 78.5714, Val Loss: 0.6683, Val Acc: 63.3333\n",
      "Epoch 56601/100000, Tr Loss: 0.4992, Tr Acc: 80.0000, Val Loss: 0.6680, Val Acc: 63.3333\n",
      "Epoch 56701/100000, Tr Loss: 0.4883, Tr Acc: 75.7143, Val Loss: 0.6683, Val Acc: 63.3333\n",
      "Epoch 56801/100000, Tr Loss: 0.4255, Tr Acc: 88.5714, Val Loss: 0.6681, Val Acc: 63.3333\n",
      "Epoch 56901/100000, Tr Loss: 0.5344, Tr Acc: 74.2857, Val Loss: 0.6680, Val Acc: 63.3333\n",
      "Epoch 57001/100000, Tr Loss: 0.5623, Tr Acc: 75.7143, Val Loss: 0.6682, Val Acc: 63.3333\n",
      "Epoch 57101/100000, Tr Loss: 0.5164, Tr Acc: 72.8571, Val Loss: 0.6680, Val Acc: 63.3333\n",
      "Epoch 57201/100000, Tr Loss: 0.5691, Tr Acc: 70.0000, Val Loss: 0.6681, Val Acc: 63.3333\n",
      "Epoch 57301/100000, Tr Loss: 0.4560, Tr Acc: 82.8571, Val Loss: 0.6679, Val Acc: 63.3333\n",
      "Epoch 57401/100000, Tr Loss: 0.4498, Tr Acc: 91.4286, Val Loss: 0.6679, Val Acc: 63.3333\n",
      "Epoch 57501/100000, Tr Loss: 0.4727, Tr Acc: 87.1429, Val Loss: 0.6678, Val Acc: 63.3333\n",
      "Epoch 57601/100000, Tr Loss: 0.4572, Tr Acc: 77.1429, Val Loss: 0.6679, Val Acc: 63.3333\n",
      "Epoch 57701/100000, Tr Loss: 0.5081, Tr Acc: 75.7143, Val Loss: 0.6678, Val Acc: 63.3333\n",
      "Epoch 57801/100000, Tr Loss: 0.5410, Tr Acc: 75.7143, Val Loss: 0.6677, Val Acc: 63.3333\n",
      "Epoch 57901/100000, Tr Loss: 0.4864, Tr Acc: 74.2857, Val Loss: 0.6677, Val Acc: 63.3333\n",
      "Epoch 58001/100000, Tr Loss: 0.5792, Tr Acc: 70.0000, Val Loss: 0.6677, Val Acc: 63.3333\n",
      "Epoch 58101/100000, Tr Loss: 0.5231, Tr Acc: 74.2857, Val Loss: 0.6676, Val Acc: 63.3333\n",
      "Epoch 58201/100000, Tr Loss: 0.4901, Tr Acc: 72.8571, Val Loss: 0.6676, Val Acc: 63.3333\n",
      "Epoch 58301/100000, Tr Loss: 0.4377, Tr Acc: 87.1429, Val Loss: 0.6675, Val Acc: 63.3333\n",
      "Epoch 58401/100000, Tr Loss: 0.5360, Tr Acc: 75.7143, Val Loss: 0.6675, Val Acc: 63.3333\n",
      "Epoch 58501/100000, Tr Loss: 0.5143, Tr Acc: 81.4286, Val Loss: 0.6675, Val Acc: 63.3333\n",
      "Epoch 58601/100000, Tr Loss: 0.4685, Tr Acc: 78.5714, Val Loss: 0.6674, Val Acc: 63.3333\n",
      "Epoch 58701/100000, Tr Loss: 0.4760, Tr Acc: 81.4286, Val Loss: 0.6673, Val Acc: 63.3333\n",
      "Epoch 58801/100000, Tr Loss: 0.5069, Tr Acc: 77.1429, Val Loss: 0.6674, Val Acc: 63.3333\n",
      "Epoch 58901/100000, Tr Loss: 0.5038, Tr Acc: 77.1429, Val Loss: 0.6671, Val Acc: 63.3333\n",
      "Epoch 59001/100000, Tr Loss: 0.4600, Tr Acc: 81.4286, Val Loss: 0.6673, Val Acc: 63.3333\n",
      "Epoch 59101/100000, Tr Loss: 0.5548, Tr Acc: 70.0000, Val Loss: 0.6672, Val Acc: 63.3333\n",
      "Epoch 59201/100000, Tr Loss: 0.5014, Tr Acc: 78.5714, Val Loss: 0.6671, Val Acc: 63.3333\n",
      "Epoch 59301/100000, Tr Loss: 0.5000, Tr Acc: 74.2857, Val Loss: 0.6671, Val Acc: 63.3333\n",
      "Epoch 59401/100000, Tr Loss: 0.5098, Tr Acc: 72.8571, Val Loss: 0.6670, Val Acc: 63.3333\n",
      "Epoch 59501/100000, Tr Loss: 0.4605, Tr Acc: 74.2857, Val Loss: 0.6672, Val Acc: 63.3333\n",
      "Epoch 59601/100000, Tr Loss: 0.4594, Tr Acc: 78.5714, Val Loss: 0.6671, Val Acc: 63.3333\n",
      "Epoch 59701/100000, Tr Loss: 0.4887, Tr Acc: 78.5714, Val Loss: 0.6670, Val Acc: 63.3333\n",
      "Epoch 59801/100000, Tr Loss: 0.5184, Tr Acc: 72.8571, Val Loss: 0.6670, Val Acc: 63.3333\n",
      "Epoch 59901/100000, Tr Loss: 0.4895, Tr Acc: 80.0000, Val Loss: 0.6669, Val Acc: 63.3333\n",
      "Epoch 60001/100000, Tr Loss: 0.5226, Tr Acc: 74.2857, Val Loss: 0.6670, Val Acc: 63.3333\n",
      "Epoch 60101/100000, Tr Loss: 0.4322, Tr Acc: 87.1429, Val Loss: 0.6669, Val Acc: 63.3333\n",
      "Epoch 60201/100000, Tr Loss: 0.5072, Tr Acc: 77.1429, Val Loss: 0.6668, Val Acc: 63.3333\n",
      "Epoch 60301/100000, Tr Loss: 0.5377, Tr Acc: 75.7143, Val Loss: 0.6669, Val Acc: 63.3333\n",
      "Epoch 60401/100000, Tr Loss: 0.4551, Tr Acc: 81.4286, Val Loss: 0.6666, Val Acc: 63.3333\n",
      "Epoch 60501/100000, Tr Loss: 0.4890, Tr Acc: 81.4286, Val Loss: 0.6666, Val Acc: 63.3333\n",
      "Epoch 60601/100000, Tr Loss: 0.5050, Tr Acc: 72.8571, Val Loss: 0.6666, Val Acc: 63.3333\n",
      "Epoch 60701/100000, Tr Loss: 0.4915, Tr Acc: 72.8571, Val Loss: 0.6667, Val Acc: 63.3333\n",
      "Epoch 60801/100000, Tr Loss: 0.5129, Tr Acc: 75.7143, Val Loss: 0.6666, Val Acc: 63.3333\n",
      "Epoch 60901/100000, Tr Loss: 0.4965, Tr Acc: 77.1429, Val Loss: 0.6664, Val Acc: 63.3333\n",
      "Epoch 61001/100000, Tr Loss: 0.4568, Tr Acc: 81.4286, Val Loss: 0.6665, Val Acc: 63.3333\n",
      "Epoch 61101/100000, Tr Loss: 0.4629, Tr Acc: 81.4286, Val Loss: 0.6666, Val Acc: 63.3333\n",
      "Epoch 61201/100000, Tr Loss: 0.5133, Tr Acc: 77.1429, Val Loss: 0.6665, Val Acc: 63.3333\n",
      "Epoch 61301/100000, Tr Loss: 0.4739, Tr Acc: 82.8571, Val Loss: 0.6664, Val Acc: 63.3333\n",
      "Epoch 61401/100000, Tr Loss: 0.5031, Tr Acc: 77.1429, Val Loss: 0.6665, Val Acc: 63.3333\n",
      "Epoch 61501/100000, Tr Loss: 0.4763, Tr Acc: 80.0000, Val Loss: 0.6662, Val Acc: 63.3333\n",
      "Epoch 61601/100000, Tr Loss: 0.4531, Tr Acc: 77.1429, Val Loss: 0.6664, Val Acc: 63.3333\n",
      "Epoch 61701/100000, Tr Loss: 0.4884, Tr Acc: 82.8571, Val Loss: 0.6662, Val Acc: 63.3333\n",
      "Epoch 61801/100000, Tr Loss: 0.4567, Tr Acc: 85.7143, Val Loss: 0.6663, Val Acc: 63.3333\n",
      "Epoch 61901/100000, Tr Loss: 0.5082, Tr Acc: 78.5714, Val Loss: 0.6662, Val Acc: 63.3333\n",
      "Epoch 62001/100000, Tr Loss: 0.4946, Tr Acc: 74.2857, Val Loss: 0.6662, Val Acc: 63.3333\n",
      "Epoch 62101/100000, Tr Loss: 0.4572, Tr Acc: 82.8571, Val Loss: 0.6662, Val Acc: 63.3333\n",
      "Epoch 62201/100000, Tr Loss: 0.4778, Tr Acc: 77.1429, Val Loss: 0.6662, Val Acc: 63.3333\n",
      "Epoch 62301/100000, Tr Loss: 0.4649, Tr Acc: 82.8571, Val Loss: 0.6659, Val Acc: 63.3333\n",
      "Epoch 62401/100000, Tr Loss: 0.5132, Tr Acc: 71.4286, Val Loss: 0.6660, Val Acc: 63.3333\n",
      "Epoch 62501/100000, Tr Loss: 0.5339, Tr Acc: 74.2857, Val Loss: 0.6659, Val Acc: 63.3333\n",
      "Epoch 62601/100000, Tr Loss: 0.4749, Tr Acc: 75.7143, Val Loss: 0.6659, Val Acc: 63.3333\n",
      "Epoch 62701/100000, Tr Loss: 0.4534, Tr Acc: 90.0000, Val Loss: 0.6660, Val Acc: 63.3333\n",
      "Epoch 62801/100000, Tr Loss: 0.4841, Tr Acc: 77.1429, Val Loss: 0.6658, Val Acc: 63.3333\n",
      "Epoch 62901/100000, Tr Loss: 0.5121, Tr Acc: 77.1429, Val Loss: 0.6659, Val Acc: 63.3333\n",
      "Epoch 63001/100000, Tr Loss: 0.5056, Tr Acc: 78.5714, Val Loss: 0.6659, Val Acc: 63.3333\n",
      "Epoch 63101/100000, Tr Loss: 0.5002, Tr Acc: 78.5714, Val Loss: 0.6659, Val Acc: 63.3333\n",
      "Epoch 63201/100000, Tr Loss: 0.4661, Tr Acc: 77.1429, Val Loss: 0.6657, Val Acc: 63.3333\n",
      "Epoch 63301/100000, Tr Loss: 0.4188, Tr Acc: 85.7143, Val Loss: 0.6656, Val Acc: 63.3333\n",
      "Epoch 63401/100000, Tr Loss: 0.4663, Tr Acc: 80.0000, Val Loss: 0.6656, Val Acc: 63.3333\n",
      "Epoch 63501/100000, Tr Loss: 0.4249, Tr Acc: 82.8571, Val Loss: 0.6658, Val Acc: 63.3333\n",
      "Epoch 63601/100000, Tr Loss: 0.3768, Tr Acc: 91.4286, Val Loss: 0.6656, Val Acc: 63.3333\n",
      "Epoch 63701/100000, Tr Loss: 0.4152, Tr Acc: 81.4286, Val Loss: 0.6655, Val Acc: 63.3333\n",
      "Epoch 63801/100000, Tr Loss: 0.4900, Tr Acc: 75.7143, Val Loss: 0.6655, Val Acc: 63.3333\n",
      "Epoch 63901/100000, Tr Loss: 0.4567, Tr Acc: 78.5714, Val Loss: 0.6654, Val Acc: 63.3333\n",
      "Epoch 64001/100000, Tr Loss: 0.5109, Tr Acc: 81.4286, Val Loss: 0.6654, Val Acc: 63.3333\n",
      "Epoch 64101/100000, Tr Loss: 0.4886, Tr Acc: 80.0000, Val Loss: 0.6656, Val Acc: 63.3333\n",
      "Epoch 64201/100000, Tr Loss: 0.4885, Tr Acc: 77.1429, Val Loss: 0.6654, Val Acc: 63.3333\n",
      "Epoch 64301/100000, Tr Loss: 0.4465, Tr Acc: 85.7143, Val Loss: 0.6652, Val Acc: 63.3333\n",
      "Epoch 64401/100000, Tr Loss: 0.4591, Tr Acc: 80.0000, Val Loss: 0.6652, Val Acc: 63.3333\n",
      "Epoch 64501/100000, Tr Loss: 0.5044, Tr Acc: 72.8571, Val Loss: 0.6651, Val Acc: 63.3333\n",
      "Epoch 64601/100000, Tr Loss: 0.4464, Tr Acc: 81.4286, Val Loss: 0.6653, Val Acc: 63.3333\n",
      "Epoch 64701/100000, Tr Loss: 0.4984, Tr Acc: 72.8571, Val Loss: 0.6651, Val Acc: 63.3333\n",
      "Epoch 64801/100000, Tr Loss: 0.4522, Tr Acc: 80.0000, Val Loss: 0.6652, Val Acc: 63.3333\n",
      "Epoch 64901/100000, Tr Loss: 0.4744, Tr Acc: 90.0000, Val Loss: 0.6652, Val Acc: 63.3333\n",
      "Epoch 65001/100000, Tr Loss: 0.4279, Tr Acc: 87.1429, Val Loss: 0.6651, Val Acc: 63.3333\n",
      "Epoch 65101/100000, Tr Loss: 0.4860, Tr Acc: 81.4286, Val Loss: 0.6650, Val Acc: 63.3333\n",
      "Epoch 65201/100000, Tr Loss: 0.5013, Tr Acc: 72.8571, Val Loss: 0.6651, Val Acc: 63.3333\n",
      "Epoch 65301/100000, Tr Loss: 0.4802, Tr Acc: 85.7143, Val Loss: 0.6650, Val Acc: 63.3333\n",
      "Epoch 65401/100000, Tr Loss: 0.4876, Tr Acc: 78.5714, Val Loss: 0.6648, Val Acc: 63.3333\n",
      "Epoch 65501/100000, Tr Loss: 0.4434, Tr Acc: 88.5714, Val Loss: 0.6648, Val Acc: 63.3333\n",
      "Epoch 65601/100000, Tr Loss: 0.4879, Tr Acc: 78.5714, Val Loss: 0.6649, Val Acc: 63.3333\n",
      "Epoch 65701/100000, Tr Loss: 0.4916, Tr Acc: 78.5714, Val Loss: 0.6650, Val Acc: 63.3333\n",
      "Epoch 65801/100000, Tr Loss: 0.5053, Tr Acc: 77.1429, Val Loss: 0.6649, Val Acc: 63.3333\n",
      "Epoch 65901/100000, Tr Loss: 0.4558, Tr Acc: 85.7143, Val Loss: 0.6647, Val Acc: 63.3333\n",
      "Epoch 66001/100000, Tr Loss: 0.4793, Tr Acc: 77.1429, Val Loss: 0.6648, Val Acc: 63.3333\n",
      "Epoch 66101/100000, Tr Loss: 0.3947, Tr Acc: 94.2857, Val Loss: 0.6648, Val Acc: 63.3333\n",
      "Epoch 66201/100000, Tr Loss: 0.4473, Tr Acc: 90.0000, Val Loss: 0.6645, Val Acc: 63.3333\n",
      "Epoch 66301/100000, Tr Loss: 0.3950, Tr Acc: 90.0000, Val Loss: 0.6646, Val Acc: 63.3333\n",
      "Epoch 66401/100000, Tr Loss: 0.4269, Tr Acc: 87.1429, Val Loss: 0.6647, Val Acc: 63.3333\n",
      "Epoch 66501/100000, Tr Loss: 0.4730, Tr Acc: 81.4286, Val Loss: 0.6645, Val Acc: 63.3333\n",
      "Epoch 66601/100000, Tr Loss: 0.4364, Tr Acc: 84.2857, Val Loss: 0.6645, Val Acc: 63.3333\n",
      "Epoch 66701/100000, Tr Loss: 0.5072, Tr Acc: 77.1429, Val Loss: 0.6645, Val Acc: 63.3333\n",
      "Epoch 66801/100000, Tr Loss: 0.5224, Tr Acc: 65.7143, Val Loss: 0.6643, Val Acc: 63.3333\n",
      "Epoch 66901/100000, Tr Loss: 0.4525, Tr Acc: 82.8571, Val Loss: 0.6643, Val Acc: 63.3333\n",
      "Epoch 67001/100000, Tr Loss: 0.4743, Tr Acc: 80.0000, Val Loss: 0.6642, Val Acc: 63.3333\n",
      "Epoch 67101/100000, Tr Loss: 0.4691, Tr Acc: 77.1429, Val Loss: 0.6645, Val Acc: 63.3333\n",
      "Epoch 67201/100000, Tr Loss: 0.4798, Tr Acc: 78.5714, Val Loss: 0.6643, Val Acc: 63.3333\n",
      "Epoch 67301/100000, Tr Loss: 0.4429, Tr Acc: 82.8571, Val Loss: 0.6643, Val Acc: 63.3333\n",
      "Epoch 67401/100000, Tr Loss: 0.4865, Tr Acc: 71.4286, Val Loss: 0.6643, Val Acc: 63.3333\n",
      "Epoch 67501/100000, Tr Loss: 0.4128, Tr Acc: 91.4286, Val Loss: 0.6643, Val Acc: 63.3333\n",
      "Epoch 67601/100000, Tr Loss: 0.5012, Tr Acc: 78.5714, Val Loss: 0.6641, Val Acc: 63.3333\n",
      "Epoch 67701/100000, Tr Loss: 0.4640, Tr Acc: 84.2857, Val Loss: 0.6640, Val Acc: 63.3333\n",
      "Epoch 67801/100000, Tr Loss: 0.4364, Tr Acc: 84.2857, Val Loss: 0.6640, Val Acc: 63.3333\n",
      "Epoch 67901/100000, Tr Loss: 0.4333, Tr Acc: 82.8571, Val Loss: 0.6640, Val Acc: 63.3333\n",
      "Epoch 68001/100000, Tr Loss: 0.4278, Tr Acc: 82.8571, Val Loss: 0.6641, Val Acc: 63.3333\n",
      "Epoch 68101/100000, Tr Loss: 0.4491, Tr Acc: 84.2857, Val Loss: 0.6641, Val Acc: 63.3333\n",
      "Epoch 68201/100000, Tr Loss: 0.4261, Tr Acc: 85.7143, Val Loss: 0.6640, Val Acc: 63.3333\n",
      "Epoch 68301/100000, Tr Loss: 0.5053, Tr Acc: 75.7143, Val Loss: 0.6640, Val Acc: 63.3333\n",
      "Epoch 68401/100000, Tr Loss: 0.4077, Tr Acc: 87.1429, Val Loss: 0.6641, Val Acc: 63.3333\n",
      "Epoch 68501/100000, Tr Loss: 0.4444, Tr Acc: 84.2857, Val Loss: 0.6638, Val Acc: 63.3333\n",
      "Epoch 68601/100000, Tr Loss: 0.5040, Tr Acc: 72.8571, Val Loss: 0.6636, Val Acc: 63.3333\n",
      "Epoch 68701/100000, Tr Loss: 0.4698, Tr Acc: 72.8571, Val Loss: 0.6639, Val Acc: 63.3333\n",
      "Epoch 68801/100000, Tr Loss: 0.5046, Tr Acc: 72.8571, Val Loss: 0.6639, Val Acc: 63.3333\n",
      "Epoch 68901/100000, Tr Loss: 0.4553, Tr Acc: 82.8571, Val Loss: 0.6636, Val Acc: 63.3333\n",
      "Epoch 69001/100000, Tr Loss: 0.4995, Tr Acc: 75.7143, Val Loss: 0.6638, Val Acc: 63.3333\n",
      "Epoch 69101/100000, Tr Loss: 0.4021, Tr Acc: 85.7143, Val Loss: 0.6637, Val Acc: 63.3333\n",
      "Epoch 69201/100000, Tr Loss: 0.4751, Tr Acc: 77.1429, Val Loss: 0.6636, Val Acc: 63.3333\n",
      "Epoch 69301/100000, Tr Loss: 0.4956, Tr Acc: 75.7143, Val Loss: 0.6638, Val Acc: 63.3333\n",
      "Epoch 69401/100000, Tr Loss: 0.3960, Tr Acc: 87.1429, Val Loss: 0.6637, Val Acc: 63.3333\n",
      "Epoch 69501/100000, Tr Loss: 0.4276, Tr Acc: 82.8571, Val Loss: 0.6636, Val Acc: 63.3333\n",
      "Epoch 69601/100000, Tr Loss: 0.4878, Tr Acc: 77.1429, Val Loss: 0.6638, Val Acc: 63.3333\n",
      "Epoch 69701/100000, Tr Loss: 0.4473, Tr Acc: 78.5714, Val Loss: 0.6636, Val Acc: 63.3333\n",
      "Epoch 69801/100000, Tr Loss: 0.4579, Tr Acc: 82.8571, Val Loss: 0.6635, Val Acc: 63.3333\n",
      "Epoch 69901/100000, Tr Loss: 0.4628, Tr Acc: 80.0000, Val Loss: 0.6634, Val Acc: 63.3333\n",
      "Epoch 70001/100000, Tr Loss: 0.4595, Tr Acc: 82.8571, Val Loss: 0.6634, Val Acc: 63.3333\n",
      "Epoch 70101/100000, Tr Loss: 0.4334, Tr Acc: 80.0000, Val Loss: 0.6633, Val Acc: 63.3333\n",
      "Epoch 70201/100000, Tr Loss: 0.4518, Tr Acc: 84.2857, Val Loss: 0.6633, Val Acc: 63.3333\n",
      "Epoch 70301/100000, Tr Loss: 0.4291, Tr Acc: 85.7143, Val Loss: 0.6634, Val Acc: 63.3333\n",
      "Epoch 70401/100000, Tr Loss: 0.3891, Tr Acc: 87.1429, Val Loss: 0.6632, Val Acc: 63.3333\n",
      "Epoch 70501/100000, Tr Loss: 0.4691, Tr Acc: 81.4286, Val Loss: 0.6631, Val Acc: 63.3333\n",
      "Epoch 70601/100000, Tr Loss: 0.4895, Tr Acc: 80.0000, Val Loss: 0.6633, Val Acc: 63.3333\n",
      "Epoch 70701/100000, Tr Loss: 0.4479, Tr Acc: 87.1429, Val Loss: 0.6633, Val Acc: 63.3333\n",
      "Epoch 70801/100000, Tr Loss: 0.4331, Tr Acc: 81.4286, Val Loss: 0.6631, Val Acc: 63.3333\n",
      "Epoch 70901/100000, Tr Loss: 0.4607, Tr Acc: 82.8571, Val Loss: 0.6633, Val Acc: 63.3333\n",
      "Epoch 71001/100000, Tr Loss: 0.4830, Tr Acc: 78.5714, Val Loss: 0.6631, Val Acc: 63.3333\n",
      "Epoch 71101/100000, Tr Loss: 0.4058, Tr Acc: 85.7143, Val Loss: 0.6632, Val Acc: 63.3333\n",
      "Epoch 71201/100000, Tr Loss: 0.4521, Tr Acc: 81.4286, Val Loss: 0.6629, Val Acc: 63.3333\n",
      "Epoch 71301/100000, Tr Loss: 0.4431, Tr Acc: 85.7143, Val Loss: 0.6630, Val Acc: 63.3333\n",
      "Epoch 71401/100000, Tr Loss: 0.4589, Tr Acc: 81.4286, Val Loss: 0.6630, Val Acc: 63.3333\n",
      "Epoch 71501/100000, Tr Loss: 0.4393, Tr Acc: 81.4286, Val Loss: 0.6631, Val Acc: 63.3333\n",
      "Epoch 71601/100000, Tr Loss: 0.4300, Tr Acc: 85.7143, Val Loss: 0.6629, Val Acc: 63.3333\n",
      "Epoch 71701/100000, Tr Loss: 0.4371, Tr Acc: 84.2857, Val Loss: 0.6630, Val Acc: 63.3333\n",
      "Epoch 71801/100000, Tr Loss: 0.4591, Tr Acc: 84.2857, Val Loss: 0.6628, Val Acc: 63.3333\n",
      "Epoch 71901/100000, Tr Loss: 0.4716, Tr Acc: 81.4286, Val Loss: 0.6629, Val Acc: 63.3333\n",
      "Epoch 72001/100000, Tr Loss: 0.4550, Tr Acc: 84.2857, Val Loss: 0.6627, Val Acc: 63.3333\n",
      "Epoch 72101/100000, Tr Loss: 0.4219, Tr Acc: 80.0000, Val Loss: 0.6627, Val Acc: 63.3333\n",
      "Epoch 72201/100000, Tr Loss: 0.4579, Tr Acc: 81.4286, Val Loss: 0.6629, Val Acc: 63.3333\n",
      "Epoch 72301/100000, Tr Loss: 0.4626, Tr Acc: 80.0000, Val Loss: 0.6627, Val Acc: 63.3333\n",
      "Epoch 72401/100000, Tr Loss: 0.4882, Tr Acc: 77.1429, Val Loss: 0.6628, Val Acc: 63.3333\n",
      "Epoch 72501/100000, Tr Loss: 0.4323, Tr Acc: 87.1429, Val Loss: 0.6626, Val Acc: 63.3333\n",
      "Epoch 72601/100000, Tr Loss: 0.4815, Tr Acc: 74.2857, Val Loss: 0.6629, Val Acc: 63.3333\n",
      "Epoch 72701/100000, Tr Loss: 0.4666, Tr Acc: 78.5714, Val Loss: 0.6627, Val Acc: 63.3333\n",
      "Epoch 72801/100000, Tr Loss: 0.4057, Tr Acc: 87.1429, Val Loss: 0.6624, Val Acc: 63.3333\n",
      "Epoch 72901/100000, Tr Loss: 0.4687, Tr Acc: 78.5714, Val Loss: 0.6628, Val Acc: 63.3333\n",
      "Epoch 73001/100000, Tr Loss: 0.4494, Tr Acc: 84.2857, Val Loss: 0.6626, Val Acc: 63.3333\n",
      "Epoch 73101/100000, Tr Loss: 0.5113, Tr Acc: 77.1429, Val Loss: 0.6626, Val Acc: 63.3333\n",
      "Epoch 73201/100000, Tr Loss: 0.4835, Tr Acc: 81.4286, Val Loss: 0.6623, Val Acc: 63.3333\n",
      "Epoch 73301/100000, Tr Loss: 0.4804, Tr Acc: 78.5714, Val Loss: 0.6625, Val Acc: 63.3333\n",
      "Epoch 73401/100000, Tr Loss: 0.4006, Tr Acc: 84.2857, Val Loss: 0.6623, Val Acc: 63.3333\n",
      "Epoch 73501/100000, Tr Loss: 0.4172, Tr Acc: 87.1429, Val Loss: 0.6623, Val Acc: 63.3333\n",
      "Epoch 73601/100000, Tr Loss: 0.4171, Tr Acc: 84.2857, Val Loss: 0.6625, Val Acc: 63.3333\n",
      "Epoch 73701/100000, Tr Loss: 0.4182, Tr Acc: 87.1429, Val Loss: 0.6624, Val Acc: 63.3333\n",
      "Epoch 73801/100000, Tr Loss: 0.3852, Tr Acc: 88.5714, Val Loss: 0.6624, Val Acc: 63.3333\n",
      "Epoch 73901/100000, Tr Loss: 0.4605, Tr Acc: 81.4286, Val Loss: 0.6623, Val Acc: 63.3333\n",
      "Epoch 74001/100000, Tr Loss: 0.4274, Tr Acc: 81.4286, Val Loss: 0.6623, Val Acc: 63.3333\n",
      "Epoch 74101/100000, Tr Loss: 0.4442, Tr Acc: 85.7143, Val Loss: 0.6623, Val Acc: 63.3333\n",
      "Epoch 74201/100000, Tr Loss: 0.4347, Tr Acc: 81.4286, Val Loss: 0.6623, Val Acc: 63.3333\n",
      "Epoch 74301/100000, Tr Loss: 0.4833, Tr Acc: 82.8571, Val Loss: 0.6622, Val Acc: 63.3333\n",
      "Epoch 74401/100000, Tr Loss: 0.4115, Tr Acc: 85.7143, Val Loss: 0.6622, Val Acc: 63.3333\n",
      "Epoch 74501/100000, Tr Loss: 0.4713, Tr Acc: 81.4286, Val Loss: 0.6620, Val Acc: 63.3333\n",
      "Epoch 74601/100000, Tr Loss: 0.4349, Tr Acc: 84.2857, Val Loss: 0.6621, Val Acc: 63.3333\n",
      "Epoch 74701/100000, Tr Loss: 0.4538, Tr Acc: 81.4286, Val Loss: 0.6621, Val Acc: 63.3333\n",
      "Epoch 74801/100000, Tr Loss: 0.4409, Tr Acc: 80.0000, Val Loss: 0.6619, Val Acc: 63.3333\n",
      "Epoch 74901/100000, Tr Loss: 0.4063, Tr Acc: 84.2857, Val Loss: 0.6619, Val Acc: 63.3333\n",
      "Epoch 75001/100000, Tr Loss: 0.4584, Tr Acc: 81.4286, Val Loss: 0.6620, Val Acc: 63.3333\n",
      "Epoch 75101/100000, Tr Loss: 0.4457, Tr Acc: 85.7143, Val Loss: 0.6620, Val Acc: 63.3333\n",
      "Epoch 75201/100000, Tr Loss: 0.4551, Tr Acc: 82.8571, Val Loss: 0.6619, Val Acc: 63.3333\n",
      "Epoch 75301/100000, Tr Loss: 0.3727, Tr Acc: 85.7143, Val Loss: 0.6621, Val Acc: 63.3333\n",
      "Epoch 75401/100000, Tr Loss: 0.4565, Tr Acc: 78.5714, Val Loss: 0.6618, Val Acc: 63.3333\n",
      "Epoch 75501/100000, Tr Loss: 0.4766, Tr Acc: 81.4286, Val Loss: 0.6619, Val Acc: 63.3333\n",
      "Epoch 75601/100000, Tr Loss: 0.4182, Tr Acc: 87.1429, Val Loss: 0.6618, Val Acc: 63.3333\n",
      "Epoch 75701/100000, Tr Loss: 0.4409, Tr Acc: 88.5714, Val Loss: 0.6619, Val Acc: 63.3333\n",
      "Epoch 75801/100000, Tr Loss: 0.4067, Tr Acc: 87.1429, Val Loss: 0.6619, Val Acc: 63.3333\n",
      "Epoch 75901/100000, Tr Loss: 0.4480, Tr Acc: 81.4286, Val Loss: 0.6618, Val Acc: 63.3333\n",
      "Epoch 76001/100000, Tr Loss: 0.4374, Tr Acc: 81.4286, Val Loss: 0.6618, Val Acc: 63.3333\n",
      "Epoch 76101/100000, Tr Loss: 0.3868, Tr Acc: 90.0000, Val Loss: 0.6617, Val Acc: 63.3333\n",
      "Epoch 76201/100000, Tr Loss: 0.4040, Tr Acc: 88.5714, Val Loss: 0.6618, Val Acc: 63.3333\n",
      "Epoch 76301/100000, Tr Loss: 0.4743, Tr Acc: 80.0000, Val Loss: 0.6617, Val Acc: 63.3333\n",
      "Epoch 76401/100000, Tr Loss: 0.4504, Tr Acc: 78.5714, Val Loss: 0.6617, Val Acc: 63.3333\n",
      "Epoch 76501/100000, Tr Loss: 0.4204, Tr Acc: 82.8571, Val Loss: 0.6618, Val Acc: 63.3333\n",
      "Epoch 76601/100000, Tr Loss: 0.3891, Tr Acc: 82.8571, Val Loss: 0.6616, Val Acc: 63.3333\n",
      "Epoch 76701/100000, Tr Loss: 0.3995, Tr Acc: 87.1429, Val Loss: 0.6617, Val Acc: 63.3333\n",
      "Epoch 76801/100000, Tr Loss: 0.3740, Tr Acc: 85.7143, Val Loss: 0.6616, Val Acc: 63.3333\n",
      "Epoch 76901/100000, Tr Loss: 0.3977, Tr Acc: 88.5714, Val Loss: 0.6616, Val Acc: 63.3333\n",
      "Epoch 77001/100000, Tr Loss: 0.4454, Tr Acc: 84.2857, Val Loss: 0.6616, Val Acc: 63.3333\n",
      "Epoch 77101/100000, Tr Loss: 0.4372, Tr Acc: 85.7143, Val Loss: 0.6616, Val Acc: 63.3333\n",
      "Epoch 77201/100000, Tr Loss: 0.4145, Tr Acc: 84.2857, Val Loss: 0.6614, Val Acc: 63.3333\n",
      "Epoch 77301/100000, Tr Loss: 0.3939, Tr Acc: 88.5714, Val Loss: 0.6614, Val Acc: 63.3333\n",
      "Epoch 77401/100000, Tr Loss: 0.4219, Tr Acc: 85.7143, Val Loss: 0.6615, Val Acc: 63.3333\n",
      "Epoch 77501/100000, Tr Loss: 0.3798, Tr Acc: 84.2857, Val Loss: 0.6614, Val Acc: 63.3333\n",
      "Epoch 77601/100000, Tr Loss: 0.3815, Tr Acc: 88.5714, Val Loss: 0.6614, Val Acc: 63.3333\n",
      "Epoch 77701/100000, Tr Loss: 0.3951, Tr Acc: 88.5714, Val Loss: 0.6614, Val Acc: 63.3333\n",
      "Epoch 77801/100000, Tr Loss: 0.4417, Tr Acc: 82.8571, Val Loss: 0.6614, Val Acc: 63.3333\n",
      "Epoch 77901/100000, Tr Loss: 0.3721, Tr Acc: 92.8571, Val Loss: 0.6614, Val Acc: 63.3333\n",
      "Epoch 78001/100000, Tr Loss: 0.4072, Tr Acc: 88.5714, Val Loss: 0.6612, Val Acc: 63.3333\n",
      "Epoch 78101/100000, Tr Loss: 0.4208, Tr Acc: 87.1429, Val Loss: 0.6612, Val Acc: 63.3333\n",
      "Epoch 78201/100000, Tr Loss: 0.4374, Tr Acc: 82.8571, Val Loss: 0.6614, Val Acc: 63.3333\n",
      "Epoch 78301/100000, Tr Loss: 0.4182, Tr Acc: 78.5714, Val Loss: 0.6611, Val Acc: 63.3333\n",
      "Epoch 78401/100000, Tr Loss: 0.3999, Tr Acc: 81.4286, Val Loss: 0.6611, Val Acc: 63.3333\n",
      "Epoch 78501/100000, Tr Loss: 0.4313, Tr Acc: 81.4286, Val Loss: 0.6613, Val Acc: 63.3333\n",
      "Epoch 78601/100000, Tr Loss: 0.4527, Tr Acc: 78.5714, Val Loss: 0.6613, Val Acc: 63.3333\n",
      "Epoch 78701/100000, Tr Loss: 0.3843, Tr Acc: 82.8571, Val Loss: 0.6612, Val Acc: 63.3333\n",
      "Epoch 78801/100000, Tr Loss: 0.3979, Tr Acc: 84.2857, Val Loss: 0.6613, Val Acc: 63.3333\n",
      "Epoch 78901/100000, Tr Loss: 0.4535, Tr Acc: 78.5714, Val Loss: 0.6612, Val Acc: 63.3333\n",
      "Epoch 79001/100000, Tr Loss: 0.4006, Tr Acc: 85.7143, Val Loss: 0.6611, Val Acc: 63.3333\n",
      "Epoch 79101/100000, Tr Loss: 0.4089, Tr Acc: 85.7143, Val Loss: 0.6611, Val Acc: 63.3333\n",
      "Epoch 79201/100000, Tr Loss: 0.4265, Tr Acc: 82.8571, Val Loss: 0.6611, Val Acc: 63.3333\n",
      "Epoch 79301/100000, Tr Loss: 0.4448, Tr Acc: 82.8571, Val Loss: 0.6610, Val Acc: 63.3333\n",
      "Epoch 79401/100000, Tr Loss: 0.4027, Tr Acc: 85.7143, Val Loss: 0.6610, Val Acc: 63.3333\n",
      "Epoch 79501/100000, Tr Loss: 0.4176, Tr Acc: 87.1429, Val Loss: 0.6611, Val Acc: 63.3333\n",
      "Epoch 79601/100000, Tr Loss: 0.4080, Tr Acc: 85.7143, Val Loss: 0.6611, Val Acc: 63.3333\n",
      "Epoch 79701/100000, Tr Loss: 0.3778, Tr Acc: 84.2857, Val Loss: 0.6610, Val Acc: 63.3333\n",
      "Epoch 79801/100000, Tr Loss: 0.4425, Tr Acc: 84.2857, Val Loss: 0.6610, Val Acc: 63.3333\n",
      "Epoch 79901/100000, Tr Loss: 0.4076, Tr Acc: 84.2857, Val Loss: 0.6610, Val Acc: 63.3333\n",
      "Epoch 80001/100000, Tr Loss: 0.4400, Tr Acc: 81.4286, Val Loss: 0.6611, Val Acc: 63.3333\n",
      "Epoch 80101/100000, Tr Loss: 0.3997, Tr Acc: 81.4286, Val Loss: 0.6609, Val Acc: 63.3333\n",
      "Epoch 80201/100000, Tr Loss: 0.4100, Tr Acc: 84.2857, Val Loss: 0.6611, Val Acc: 63.3333\n",
      "Epoch 80301/100000, Tr Loss: 0.4416, Tr Acc: 81.4286, Val Loss: 0.6609, Val Acc: 63.3333\n",
      "Epoch 80401/100000, Tr Loss: 0.4010, Tr Acc: 88.5714, Val Loss: 0.6608, Val Acc: 63.3333\n",
      "Epoch 80501/100000, Tr Loss: 0.4426, Tr Acc: 81.4286, Val Loss: 0.6607, Val Acc: 63.3333\n",
      "Epoch 80601/100000, Tr Loss: 0.4112, Tr Acc: 85.7143, Val Loss: 0.6609, Val Acc: 63.3333\n",
      "Epoch 80701/100000, Tr Loss: 0.4107, Tr Acc: 84.2857, Val Loss: 0.6607, Val Acc: 63.3333\n",
      "Epoch 80801/100000, Tr Loss: 0.4309, Tr Acc: 84.2857, Val Loss: 0.6607, Val Acc: 63.3333\n",
      "Epoch 80901/100000, Tr Loss: 0.4290, Tr Acc: 82.8571, Val Loss: 0.6608, Val Acc: 63.3333\n",
      "Epoch 81001/100000, Tr Loss: 0.4506, Tr Acc: 81.4286, Val Loss: 0.6609, Val Acc: 63.3333\n",
      "Epoch 81101/100000, Tr Loss: 0.4096, Tr Acc: 80.0000, Val Loss: 0.6608, Val Acc: 63.3333\n",
      "Epoch 81201/100000, Tr Loss: 0.3914, Tr Acc: 82.8571, Val Loss: 0.6606, Val Acc: 63.3333\n",
      "Epoch 81301/100000, Tr Loss: 0.3401, Tr Acc: 95.7143, Val Loss: 0.6608, Val Acc: 63.3333\n",
      "Epoch 81401/100000, Tr Loss: 0.4234, Tr Acc: 81.4286, Val Loss: 0.6605, Val Acc: 63.3333\n",
      "Epoch 81501/100000, Tr Loss: 0.4301, Tr Acc: 81.4286, Val Loss: 0.6607, Val Acc: 63.3333\n",
      "Epoch 81601/100000, Tr Loss: 0.3918, Tr Acc: 88.5714, Val Loss: 0.6607, Val Acc: 63.3333\n",
      "Epoch 81701/100000, Tr Loss: 0.4534, Tr Acc: 78.5714, Val Loss: 0.6607, Val Acc: 63.3333\n",
      "Epoch 81801/100000, Tr Loss: 0.4077, Tr Acc: 84.2857, Val Loss: 0.6605, Val Acc: 63.3333\n",
      "Epoch 81901/100000, Tr Loss: 0.4160, Tr Acc: 87.1429, Val Loss: 0.6604, Val Acc: 63.3333\n",
      "Epoch 82001/100000, Tr Loss: 0.4530, Tr Acc: 84.2857, Val Loss: 0.6606, Val Acc: 63.3333\n",
      "Epoch 82101/100000, Tr Loss: 0.3866, Tr Acc: 87.1429, Val Loss: 0.6604, Val Acc: 63.3333\n",
      "Epoch 82201/100000, Tr Loss: 0.3876, Tr Acc: 88.5714, Val Loss: 0.6605, Val Acc: 63.3333\n",
      "Epoch 82301/100000, Tr Loss: 0.4208, Tr Acc: 85.7143, Val Loss: 0.6606, Val Acc: 63.3333\n",
      "Epoch 82401/100000, Tr Loss: 0.4070, Tr Acc: 87.1429, Val Loss: 0.6604, Val Acc: 63.3333\n",
      "Epoch 82501/100000, Tr Loss: 0.4460, Tr Acc: 77.1429, Val Loss: 0.6604, Val Acc: 63.3333\n",
      "Epoch 82601/100000, Tr Loss: 0.3896, Tr Acc: 90.0000, Val Loss: 0.6604, Val Acc: 63.3333\n",
      "Epoch 82701/100000, Tr Loss: 0.4273, Tr Acc: 81.4286, Val Loss: 0.6603, Val Acc: 63.3333\n",
      "Epoch 82801/100000, Tr Loss: 0.4008, Tr Acc: 84.2857, Val Loss: 0.6604, Val Acc: 63.3333\n",
      "Epoch 82901/100000, Tr Loss: 0.3967, Tr Acc: 81.4286, Val Loss: 0.6604, Val Acc: 63.3333\n",
      "Epoch 83001/100000, Tr Loss: 0.3986, Tr Acc: 85.7143, Val Loss: 0.6603, Val Acc: 63.3333\n",
      "Epoch 83101/100000, Tr Loss: 0.3874, Tr Acc: 88.5714, Val Loss: 0.6604, Val Acc: 63.3333\n",
      "Epoch 83201/100000, Tr Loss: 0.4042, Tr Acc: 90.0000, Val Loss: 0.6601, Val Acc: 63.3333\n",
      "Epoch 83301/100000, Tr Loss: 0.4073, Tr Acc: 81.4286, Val Loss: 0.6605, Val Acc: 63.3333\n",
      "Epoch 83401/100000, Tr Loss: 0.4255, Tr Acc: 85.7143, Val Loss: 0.6602, Val Acc: 63.3333\n",
      "Epoch 83501/100000, Tr Loss: 0.4171, Tr Acc: 85.7143, Val Loss: 0.6600, Val Acc: 63.3333\n",
      "Epoch 83601/100000, Tr Loss: 0.4289, Tr Acc: 87.1429, Val Loss: 0.6603, Val Acc: 63.3333\n",
      "Epoch 83701/100000, Tr Loss: 0.3881, Tr Acc: 82.8571, Val Loss: 0.6603, Val Acc: 63.3333\n",
      "Epoch 83801/100000, Tr Loss: 0.4124, Tr Acc: 80.0000, Val Loss: 0.6602, Val Acc: 63.3333\n",
      "Epoch 83901/100000, Tr Loss: 0.4046, Tr Acc: 88.5714, Val Loss: 0.6602, Val Acc: 63.3333\n",
      "Epoch 84001/100000, Tr Loss: 0.3898, Tr Acc: 87.1429, Val Loss: 0.6601, Val Acc: 63.3333\n",
      "Epoch 84101/100000, Tr Loss: 0.3728, Tr Acc: 88.5714, Val Loss: 0.6602, Val Acc: 63.3333\n",
      "Epoch 84201/100000, Tr Loss: 0.3823, Tr Acc: 88.5714, Val Loss: 0.6600, Val Acc: 63.3333\n",
      "Epoch 84301/100000, Tr Loss: 0.4190, Tr Acc: 87.1429, Val Loss: 0.6602, Val Acc: 63.3333\n",
      "Epoch 84401/100000, Tr Loss: 0.3855, Tr Acc: 87.1429, Val Loss: 0.6601, Val Acc: 63.3333\n",
      "Epoch 84501/100000, Tr Loss: 0.3504, Tr Acc: 91.4286, Val Loss: 0.6601, Val Acc: 63.3333\n",
      "Epoch 84601/100000, Tr Loss: 0.3934, Tr Acc: 84.2857, Val Loss: 0.6600, Val Acc: 63.3333\n",
      "Epoch 84701/100000, Tr Loss: 0.4072, Tr Acc: 82.8571, Val Loss: 0.6601, Val Acc: 63.3333\n",
      "Epoch 84801/100000, Tr Loss: 0.4077, Tr Acc: 82.8571, Val Loss: 0.6600, Val Acc: 63.3333\n",
      "Epoch 84901/100000, Tr Loss: 0.3463, Tr Acc: 91.4286, Val Loss: 0.6599, Val Acc: 63.3333\n",
      "Epoch 85001/100000, Tr Loss: 0.4016, Tr Acc: 82.8571, Val Loss: 0.6599, Val Acc: 63.3333\n",
      "Epoch 85101/100000, Tr Loss: 0.4310, Tr Acc: 77.1429, Val Loss: 0.6599, Val Acc: 63.3333\n",
      "Epoch 85201/100000, Tr Loss: 0.3493, Tr Acc: 87.1429, Val Loss: 0.6602, Val Acc: 63.3333\n",
      "Epoch 85301/100000, Tr Loss: 0.3950, Tr Acc: 88.5714, Val Loss: 0.6599, Val Acc: 63.3333\n",
      "Epoch 85401/100000, Tr Loss: 0.4341, Tr Acc: 84.2857, Val Loss: 0.6600, Val Acc: 63.3333\n",
      "Epoch 85501/100000, Tr Loss: 0.3908, Tr Acc: 87.1429, Val Loss: 0.6600, Val Acc: 63.3333\n",
      "Epoch 85601/100000, Tr Loss: 0.4082, Tr Acc: 85.7143, Val Loss: 0.6599, Val Acc: 63.3333\n",
      "Epoch 85701/100000, Tr Loss: 0.3979, Tr Acc: 85.7143, Val Loss: 0.6597, Val Acc: 63.3333\n",
      "Epoch 85801/100000, Tr Loss: 0.3519, Tr Acc: 90.0000, Val Loss: 0.6599, Val Acc: 63.3333\n",
      "Epoch 85901/100000, Tr Loss: 0.3761, Tr Acc: 87.1429, Val Loss: 0.6600, Val Acc: 63.3333\n",
      "Epoch 86001/100000, Tr Loss: 0.3914, Tr Acc: 90.0000, Val Loss: 0.6598, Val Acc: 63.3333\n",
      "Epoch 86101/100000, Tr Loss: 0.3835, Tr Acc: 90.0000, Val Loss: 0.6597, Val Acc: 63.3333\n",
      "Epoch 86201/100000, Tr Loss: 0.4143, Tr Acc: 85.7143, Val Loss: 0.6600, Val Acc: 63.3333\n",
      "Epoch 86301/100000, Tr Loss: 0.4468, Tr Acc: 78.5714, Val Loss: 0.6598, Val Acc: 63.3333\n",
      "Epoch 86401/100000, Tr Loss: 0.4139, Tr Acc: 84.2857, Val Loss: 0.6597, Val Acc: 63.3333\n",
      "Epoch 86501/100000, Tr Loss: 0.3932, Tr Acc: 88.5714, Val Loss: 0.6597, Val Acc: 63.3333\n",
      "Epoch 86601/100000, Tr Loss: 0.4174, Tr Acc: 90.0000, Val Loss: 0.6596, Val Acc: 63.3333\n",
      "Epoch 86701/100000, Tr Loss: 0.4129, Tr Acc: 87.1429, Val Loss: 0.6598, Val Acc: 63.3333\n",
      "Epoch 86801/100000, Tr Loss: 0.3770, Tr Acc: 90.0000, Val Loss: 0.6598, Val Acc: 63.3333\n",
      "Epoch 86901/100000, Tr Loss: 0.4232, Tr Acc: 90.0000, Val Loss: 0.6597, Val Acc: 63.3333\n",
      "Epoch 87001/100000, Tr Loss: 0.4346, Tr Acc: 82.8571, Val Loss: 0.6596, Val Acc: 63.3333\n",
      "Epoch 87101/100000, Tr Loss: 0.3799, Tr Acc: 91.4286, Val Loss: 0.6596, Val Acc: 63.3333\n",
      "Epoch 87201/100000, Tr Loss: 0.3892, Tr Acc: 87.1429, Val Loss: 0.6596, Val Acc: 63.3333\n",
      "Epoch 87301/100000, Tr Loss: 0.3953, Tr Acc: 84.2857, Val Loss: 0.6596, Val Acc: 63.3333\n",
      "Epoch 87401/100000, Tr Loss: 0.3974, Tr Acc: 87.1429, Val Loss: 0.6597, Val Acc: 63.3333\n",
      "Epoch 87501/100000, Tr Loss: 0.3900, Tr Acc: 85.7143, Val Loss: 0.6596, Val Acc: 63.3333\n",
      "Epoch 87601/100000, Tr Loss: 0.4051, Tr Acc: 88.5714, Val Loss: 0.6596, Val Acc: 63.3333\n",
      "Epoch 87701/100000, Tr Loss: 0.3937, Tr Acc: 80.0000, Val Loss: 0.6594, Val Acc: 63.3333\n",
      "Epoch 87801/100000, Tr Loss: 0.3493, Tr Acc: 91.4286, Val Loss: 0.6596, Val Acc: 63.3333\n",
      "Epoch 87901/100000, Tr Loss: 0.3926, Tr Acc: 85.7143, Val Loss: 0.6595, Val Acc: 63.3333\n",
      "Epoch 88001/100000, Tr Loss: 0.4043, Tr Acc: 87.1429, Val Loss: 0.6596, Val Acc: 63.3333\n",
      "Epoch 88101/100000, Tr Loss: 0.3985, Tr Acc: 82.8571, Val Loss: 0.6595, Val Acc: 63.3333\n",
      "Epoch 88201/100000, Tr Loss: 0.3891, Tr Acc: 88.5714, Val Loss: 0.6596, Val Acc: 63.3333\n",
      "Epoch 88301/100000, Tr Loss: 0.3953, Tr Acc: 84.2857, Val Loss: 0.6596, Val Acc: 63.3333\n",
      "Epoch 88401/100000, Tr Loss: 0.3937, Tr Acc: 87.1429, Val Loss: 0.6596, Val Acc: 63.3333\n",
      "Epoch 88501/100000, Tr Loss: 0.4015, Tr Acc: 87.1429, Val Loss: 0.6595, Val Acc: 63.3333\n",
      "Epoch 88601/100000, Tr Loss: 0.4154, Tr Acc: 84.2857, Val Loss: 0.6595, Val Acc: 63.3333\n",
      "Epoch 88701/100000, Tr Loss: 0.3815, Tr Acc: 84.2857, Val Loss: 0.6595, Val Acc: 66.6667\n",
      "Epoch 88801/100000, Tr Loss: 0.3537, Tr Acc: 92.8571, Val Loss: 0.6594, Val Acc: 66.6667\n",
      "Epoch 88901/100000, Tr Loss: 0.4078, Tr Acc: 88.5714, Val Loss: 0.6595, Val Acc: 66.6667\n",
      "Epoch 89001/100000, Tr Loss: 0.3713, Tr Acc: 88.5714, Val Loss: 0.6595, Val Acc: 66.6667\n",
      "Epoch 89101/100000, Tr Loss: 0.3534, Tr Acc: 92.8571, Val Loss: 0.6594, Val Acc: 66.6667\n",
      "Epoch 89201/100000, Tr Loss: 0.4061, Tr Acc: 87.1429, Val Loss: 0.6594, Val Acc: 66.6667\n",
      "Epoch 89301/100000, Tr Loss: 0.3736, Tr Acc: 88.5714, Val Loss: 0.6594, Val Acc: 66.6667\n",
      "Epoch 89401/100000, Tr Loss: 0.4112, Tr Acc: 88.5714, Val Loss: 0.6594, Val Acc: 66.6667\n",
      "Epoch 89501/100000, Tr Loss: 0.3502, Tr Acc: 94.2857, Val Loss: 0.6594, Val Acc: 66.6667\n",
      "Epoch 89601/100000, Tr Loss: 0.3997, Tr Acc: 88.5714, Val Loss: 0.6592, Val Acc: 66.6667\n",
      "Epoch 89701/100000, Tr Loss: 0.3779, Tr Acc: 84.2857, Val Loss: 0.6594, Val Acc: 63.3333\n",
      "Epoch 89801/100000, Tr Loss: 0.3578, Tr Acc: 88.5714, Val Loss: 0.6594, Val Acc: 66.6667\n",
      "Epoch 89901/100000, Tr Loss: 0.3924, Tr Acc: 84.2857, Val Loss: 0.6594, Val Acc: 66.6667\n",
      "Epoch 90001/100000, Tr Loss: 0.3462, Tr Acc: 91.4286, Val Loss: 0.6595, Val Acc: 66.6667\n",
      "Epoch 90101/100000, Tr Loss: 0.4140, Tr Acc: 85.7143, Val Loss: 0.6593, Val Acc: 66.6667\n",
      "Epoch 90201/100000, Tr Loss: 0.4528, Tr Acc: 85.7143, Val Loss: 0.6594, Val Acc: 66.6667\n",
      "Epoch 90301/100000, Tr Loss: 0.4072, Tr Acc: 84.2857, Val Loss: 0.6593, Val Acc: 66.6667\n",
      "Epoch 90401/100000, Tr Loss: 0.3768, Tr Acc: 87.1429, Val Loss: 0.6593, Val Acc: 66.6667\n",
      "Epoch 90501/100000, Tr Loss: 0.4465, Tr Acc: 84.2857, Val Loss: 0.6593, Val Acc: 66.6667\n",
      "Epoch 90601/100000, Tr Loss: 0.3789, Tr Acc: 85.7143, Val Loss: 0.6591, Val Acc: 66.6667\n",
      "Epoch 90701/100000, Tr Loss: 0.3833, Tr Acc: 84.2857, Val Loss: 0.6592, Val Acc: 66.6667\n",
      "Epoch 90801/100000, Tr Loss: 0.4034, Tr Acc: 82.8571, Val Loss: 0.6592, Val Acc: 66.6667\n",
      "Epoch 90901/100000, Tr Loss: 0.3718, Tr Acc: 91.4286, Val Loss: 0.6592, Val Acc: 66.6667\n",
      "Epoch 91001/100000, Tr Loss: 0.3547, Tr Acc: 87.1429, Val Loss: 0.6592, Val Acc: 66.6667\n",
      "Epoch 91101/100000, Tr Loss: 0.3897, Tr Acc: 84.2857, Val Loss: 0.6593, Val Acc: 66.6667\n",
      "Epoch 91201/100000, Tr Loss: 0.3516, Tr Acc: 91.4286, Val Loss: 0.6592, Val Acc: 66.6667\n",
      "Epoch 91301/100000, Tr Loss: 0.3796, Tr Acc: 90.0000, Val Loss: 0.6593, Val Acc: 66.6667\n",
      "Epoch 91401/100000, Tr Loss: 0.4099, Tr Acc: 87.1429, Val Loss: 0.6592, Val Acc: 66.6667\n",
      "Epoch 91501/100000, Tr Loss: 0.3245, Tr Acc: 90.0000, Val Loss: 0.6592, Val Acc: 66.6667\n",
      "Epoch 91601/100000, Tr Loss: 0.3310, Tr Acc: 92.8571, Val Loss: 0.6592, Val Acc: 66.6667\n",
      "Epoch 91701/100000, Tr Loss: 0.3619, Tr Acc: 88.5714, Val Loss: 0.6592, Val Acc: 66.6667\n",
      "Epoch 91801/100000, Tr Loss: 0.3614, Tr Acc: 90.0000, Val Loss: 0.6589, Val Acc: 66.6667\n",
      "Epoch 91901/100000, Tr Loss: 0.3844, Tr Acc: 87.1429, Val Loss: 0.6590, Val Acc: 66.6667\n",
      "Epoch 92001/100000, Tr Loss: 0.3914, Tr Acc: 88.5714, Val Loss: 0.6591, Val Acc: 66.6667\n",
      "Epoch 92101/100000, Tr Loss: 0.4082, Tr Acc: 85.7143, Val Loss: 0.6590, Val Acc: 66.6667\n",
      "Epoch 92201/100000, Tr Loss: 0.3732, Tr Acc: 88.5714, Val Loss: 0.6590, Val Acc: 66.6667\n",
      "Epoch 92301/100000, Tr Loss: 0.4351, Tr Acc: 82.8571, Val Loss: 0.6592, Val Acc: 66.6667\n",
      "Epoch 92401/100000, Tr Loss: 0.3911, Tr Acc: 81.4286, Val Loss: 0.6590, Val Acc: 66.6667\n",
      "Epoch 92501/100000, Tr Loss: 0.3962, Tr Acc: 81.4286, Val Loss: 0.6591, Val Acc: 66.6667\n",
      "Epoch 92601/100000, Tr Loss: 0.3571, Tr Acc: 88.5714, Val Loss: 0.6590, Val Acc: 66.6667\n",
      "Epoch 92701/100000, Tr Loss: 0.3560, Tr Acc: 90.0000, Val Loss: 0.6590, Val Acc: 66.6667\n",
      "Epoch 92801/100000, Tr Loss: 0.3587, Tr Acc: 88.5714, Val Loss: 0.6590, Val Acc: 66.6667\n",
      "Epoch 92901/100000, Tr Loss: 0.4129, Tr Acc: 85.7143, Val Loss: 0.6591, Val Acc: 66.6667\n",
      "Epoch 93001/100000, Tr Loss: 0.3324, Tr Acc: 94.2857, Val Loss: 0.6590, Val Acc: 66.6667\n",
      "Epoch 93101/100000, Tr Loss: 0.3340, Tr Acc: 94.2857, Val Loss: 0.6591, Val Acc: 66.6667\n",
      "Epoch 93201/100000, Tr Loss: 0.3475, Tr Acc: 91.4286, Val Loss: 0.6590, Val Acc: 66.6667\n",
      "Epoch 93301/100000, Tr Loss: 0.4231, Tr Acc: 81.4286, Val Loss: 0.6589, Val Acc: 66.6667\n",
      "Epoch 93401/100000, Tr Loss: 0.3894, Tr Acc: 85.7143, Val Loss: 0.6591, Val Acc: 66.6667\n",
      "Epoch 93501/100000, Tr Loss: 0.3823, Tr Acc: 91.4286, Val Loss: 0.6590, Val Acc: 66.6667\n",
      "Epoch 93601/100000, Tr Loss: 0.4072, Tr Acc: 88.5714, Val Loss: 0.6590, Val Acc: 66.6667\n",
      "Epoch 93701/100000, Tr Loss: 0.3743, Tr Acc: 87.1429, Val Loss: 0.6588, Val Acc: 66.6667\n",
      "Epoch 93801/100000, Tr Loss: 0.3448, Tr Acc: 92.8571, Val Loss: 0.6589, Val Acc: 66.6667\n",
      "Epoch 93901/100000, Tr Loss: 0.4060, Tr Acc: 81.4286, Val Loss: 0.6592, Val Acc: 66.6667\n",
      "Epoch 94001/100000, Tr Loss: 0.3497, Tr Acc: 88.5714, Val Loss: 0.6591, Val Acc: 66.6667\n",
      "Epoch 94101/100000, Tr Loss: 0.3961, Tr Acc: 88.5714, Val Loss: 0.6590, Val Acc: 66.6667\n",
      "Epoch 94201/100000, Tr Loss: 0.3748, Tr Acc: 88.5714, Val Loss: 0.6589, Val Acc: 66.6667\n",
      "Epoch 94301/100000, Tr Loss: 0.3818, Tr Acc: 85.7143, Val Loss: 0.6590, Val Acc: 66.6667\n",
      "Epoch 94401/100000, Tr Loss: 0.3592, Tr Acc: 87.1429, Val Loss: 0.6589, Val Acc: 66.6667\n",
      "Epoch 94501/100000, Tr Loss: 0.4150, Tr Acc: 85.7143, Val Loss: 0.6589, Val Acc: 66.6667\n",
      "Epoch 94601/100000, Tr Loss: 0.3364, Tr Acc: 90.0000, Val Loss: 0.6589, Val Acc: 66.6667\n",
      "Epoch 94701/100000, Tr Loss: 0.3668, Tr Acc: 88.5714, Val Loss: 0.6589, Val Acc: 66.6667\n",
      "Epoch 94801/100000, Tr Loss: 0.3498, Tr Acc: 87.1429, Val Loss: 0.6589, Val Acc: 66.6667\n",
      "Epoch 94901/100000, Tr Loss: 0.3940, Tr Acc: 88.5714, Val Loss: 0.6591, Val Acc: 66.6667\n",
      "Epoch 95001/100000, Tr Loss: 0.3834, Tr Acc: 87.1429, Val Loss: 0.6589, Val Acc: 66.6667\n",
      "Epoch 95101/100000, Tr Loss: 0.3595, Tr Acc: 91.4286, Val Loss: 0.6589, Val Acc: 66.6667\n",
      "Epoch 95201/100000, Tr Loss: 0.3697, Tr Acc: 85.7143, Val Loss: 0.6589, Val Acc: 66.6667\n",
      "Epoch 95301/100000, Tr Loss: 0.4184, Tr Acc: 80.0000, Val Loss: 0.6588, Val Acc: 66.6667\n",
      "Epoch 95401/100000, Tr Loss: 0.3720, Tr Acc: 88.5714, Val Loss: 0.6588, Val Acc: 66.6667\n",
      "Epoch 95501/100000, Tr Loss: 0.3406, Tr Acc: 90.0000, Val Loss: 0.6589, Val Acc: 66.6667\n",
      "Epoch 95601/100000, Tr Loss: 0.4105, Tr Acc: 87.1429, Val Loss: 0.6587, Val Acc: 66.6667\n",
      "Epoch 95701/100000, Tr Loss: 0.3567, Tr Acc: 90.0000, Val Loss: 0.6590, Val Acc: 66.6667\n",
      "Epoch 95801/100000, Tr Loss: 0.4405, Tr Acc: 82.8571, Val Loss: 0.6590, Val Acc: 66.6667\n",
      "Epoch 95901/100000, Tr Loss: 0.4097, Tr Acc: 87.1429, Val Loss: 0.6588, Val Acc: 66.6667\n",
      "Epoch 96001/100000, Tr Loss: 0.3924, Tr Acc: 84.2857, Val Loss: 0.6587, Val Acc: 66.6667\n",
      "Epoch 96101/100000, Tr Loss: 0.3315, Tr Acc: 94.2857, Val Loss: 0.6589, Val Acc: 66.6667\n",
      "Epoch 96201/100000, Tr Loss: 0.3522, Tr Acc: 87.1429, Val Loss: 0.6589, Val Acc: 66.6667\n",
      "Epoch 96301/100000, Tr Loss: 0.3771, Tr Acc: 84.2857, Val Loss: 0.6590, Val Acc: 66.6667\n",
      "Epoch 96401/100000, Tr Loss: 0.3942, Tr Acc: 87.1429, Val Loss: 0.6588, Val Acc: 66.6667\n",
      "Epoch 96501/100000, Tr Loss: 0.3221, Tr Acc: 95.7143, Val Loss: 0.6588, Val Acc: 66.6667\n",
      "Epoch 96601/100000, Tr Loss: 0.3413, Tr Acc: 90.0000, Val Loss: 0.6588, Val Acc: 66.6667\n",
      "Epoch 96701/100000, Tr Loss: 0.3729, Tr Acc: 82.8571, Val Loss: 0.6588, Val Acc: 66.6667\n",
      "Epoch 96801/100000, Tr Loss: 0.3849, Tr Acc: 85.7143, Val Loss: 0.6589, Val Acc: 66.6667\n",
      "Epoch 96901/100000, Tr Loss: 0.3609, Tr Acc: 90.0000, Val Loss: 0.6589, Val Acc: 66.6667\n",
      "Epoch 97001/100000, Tr Loss: 0.4059, Tr Acc: 82.8571, Val Loss: 0.6589, Val Acc: 66.6667\n",
      "Epoch 97101/100000, Tr Loss: 0.3530, Tr Acc: 90.0000, Val Loss: 0.6589, Val Acc: 66.6667\n",
      "Epoch 97201/100000, Tr Loss: 0.3575, Tr Acc: 91.4286, Val Loss: 0.6587, Val Acc: 66.6667\n",
      "Epoch 97301/100000, Tr Loss: 0.3531, Tr Acc: 85.7143, Val Loss: 0.6589, Val Acc: 66.6667\n",
      "Epoch 97401/100000, Tr Loss: 0.3300, Tr Acc: 92.8571, Val Loss: 0.6587, Val Acc: 66.6667\n",
      "Epoch 97501/100000, Tr Loss: 0.3712, Tr Acc: 88.5714, Val Loss: 0.6586, Val Acc: 66.6667\n",
      "Epoch 97601/100000, Tr Loss: 0.3614, Tr Acc: 90.0000, Val Loss: 0.6589, Val Acc: 66.6667\n",
      "Epoch 97701/100000, Tr Loss: 0.3567, Tr Acc: 84.2857, Val Loss: 0.6588, Val Acc: 66.6667\n",
      "Epoch 97801/100000, Tr Loss: 0.3962, Tr Acc: 87.1429, Val Loss: 0.6588, Val Acc: 66.6667\n",
      "Epoch 97901/100000, Tr Loss: 0.3403, Tr Acc: 90.0000, Val Loss: 0.6588, Val Acc: 66.6667\n",
      "Epoch 98001/100000, Tr Loss: 0.3182, Tr Acc: 88.5714, Val Loss: 0.6589, Val Acc: 66.6667\n",
      "Epoch 98101/100000, Tr Loss: 0.3669, Tr Acc: 90.0000, Val Loss: 0.6588, Val Acc: 66.6667\n",
      "Epoch 98201/100000, Tr Loss: 0.3252, Tr Acc: 91.4286, Val Loss: 0.6587, Val Acc: 66.6667\n",
      "Epoch 98301/100000, Tr Loss: 0.3762, Tr Acc: 82.8571, Val Loss: 0.6587, Val Acc: 66.6667\n",
      "Epoch 98401/100000, Tr Loss: 0.3365, Tr Acc: 90.0000, Val Loss: 0.6587, Val Acc: 66.6667\n",
      "Epoch 98501/100000, Tr Loss: 0.3354, Tr Acc: 92.8571, Val Loss: 0.6587, Val Acc: 66.6667\n",
      "Epoch 98601/100000, Tr Loss: 0.3808, Tr Acc: 85.7143, Val Loss: 0.6586, Val Acc: 66.6667\n",
      "Epoch 98701/100000, Tr Loss: 0.4207, Tr Acc: 81.4286, Val Loss: 0.6587, Val Acc: 66.6667\n",
      "Epoch 98801/100000, Tr Loss: 0.3438, Tr Acc: 91.4286, Val Loss: 0.6588, Val Acc: 66.6667\n",
      "Epoch 98901/100000, Tr Loss: 0.3620, Tr Acc: 85.7143, Val Loss: 0.6587, Val Acc: 66.6667\n",
      "Epoch 99001/100000, Tr Loss: 0.3605, Tr Acc: 88.5714, Val Loss: 0.6587, Val Acc: 66.6667\n",
      "Epoch 99101/100000, Tr Loss: 0.3359, Tr Acc: 92.8571, Val Loss: 0.6588, Val Acc: 66.6667\n",
      "Epoch 99201/100000, Tr Loss: 0.3666, Tr Acc: 84.2857, Val Loss: 0.6588, Val Acc: 66.6667\n",
      "Epoch 99301/100000, Tr Loss: 0.3417, Tr Acc: 94.2857, Val Loss: 0.6587, Val Acc: 66.6667\n",
      "Epoch 99401/100000, Tr Loss: 0.3704, Tr Acc: 90.0000, Val Loss: 0.6588, Val Acc: 66.6667\n",
      "Epoch 99501/100000, Tr Loss: 0.3531, Tr Acc: 92.8571, Val Loss: 0.6590, Val Acc: 66.6667\n",
      "Epoch 99601/100000, Tr Loss: 0.3412, Tr Acc: 87.1429, Val Loss: 0.6587, Val Acc: 66.6667\n",
      "Epoch 99701/100000, Tr Loss: 0.3390, Tr Acc: 90.0000, Val Loss: 0.6587, Val Acc: 66.6667\n",
      "Epoch 99801/100000, Tr Loss: 0.3411, Tr Acc: 87.1429, Val Loss: 0.6587, Val Acc: 66.6667\n",
      "Epoch 99901/100000, Tr Loss: 0.3343, Tr Acc: 90.0000, Val Loss: 0.6587, Val Acc: 66.6667\n"
     ]
    }
   ],
   "source": [
    "from utils import train\n",
    "config = wand.config\n",
    "optimizer = optim.Adam(net.parameters(), lr=config.learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = 'cuda'\n",
    "\n",
    "train_loss,valid_loss,train_accuracy,valid_accuracy =train(\n",
    "    model = net,\n",
    "    loader_train = train_loader,\n",
    "    loader_test = test_loader,\n",
    "    vail_loader = None,\n",
    "    optimizer = optimizer  ,\n",
    "    criterion = criterion ,\n",
    "    device = 'cuda',\n",
    "    wand = wand\n",
    ")\n",
    "\n",
    "\n",
    "wandb.alert(\n",
    "            title='Finish',\n",
    "            text=f'Finishing training',\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000, 1])\n",
      "torch.Size([100])\n",
      "torch.Size([5000, 1, 200])\n",
      "torch.Size([5000, 1, 200])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torchsummary import summary\n",
    "from transformer import Transformer\n",
    " \n",
    "\n",
    "sequence_len=1750 # sequence length of time series\n",
    "max_len=5000 # max time series sequence length \n",
    "n_head = 4 # number of attention head\n",
    "n_layer = 2 # number of encoder layer\n",
    "drop_prob = 0.1\n",
    "d_model = 200 # number of dimension ( for positional embedding)\n",
    "ffn_hidden = 512 # size of hidden layer before classification \n",
    "in_features = 3 # for univariate time series (1d), it must be adjusted for 1. \n",
    "n_classes = 2\n",
    "model =  Transformer( in_features=in_features,\n",
    "                     d_model=d_model,\n",
    "                     details=False,\n",
    "                     n_head=n_head,\n",
    "                     max_len=max_len,\n",
    "                     seq_len=sequence_len,\n",
    "                     ffn_hidden=ffn_hidden,\n",
    "                     n_layers=n_layer,\n",
    "                     drop_prob=drop_prob,\n",
    "                     n_classes=n_classes,\n",
    "                     device=device)\n",
    "\n",
    "batch_size = 555\n",
    "\n",
    "#summary(net, (2, 641),32)\n",
    "#summary(model, input_size=(batch_size,sequence_len,feature))\n",
    "\n",
    "input_ = torch.from_numpy(np.empty((batch_size,sequence_len,in_features))).float()\n",
    "\n",
    "model(input_)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "71ee62090f476f7f208daa0d546a5a64db59508b1a22febc715667ce49424855"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
