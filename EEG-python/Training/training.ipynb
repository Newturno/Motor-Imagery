{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import pywt\n",
    "from mne.time_frequency import tfr_multitaper\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.cuda as cuda\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "\n",
    "from utils import create_dataloader,train,bandpower\n",
    "from dataset import EEG\n",
    "import wandb\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "# Now do your import\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/pongkornsettasompop/Desktop/work/Motor-Imagery/EEG-python/dataset/recorded_EEG\n",
      "Raw done\n",
      "(9, 47445)\n",
      "250.0\n"
     ]
    }
   ],
   "source": [
    "# home directory + datasets folder\n",
    "#C:\\Users\\Pongk\\Desktop\\Work\\mi-project\\EEG-python\\dataset\\recorded_EEG\n",
    "path = RECORED_PATH\n",
    "#path = \"/root/EEG_Model/dataset/finetune_EEG/\"\n",
    "#subject to run\n",
    "runs = [1]\n",
    "#runs = [1,2,3,4,12,13,14,15,16,17]\n",
    "#runs = [1]\n",
    "#runs = [7,8,9,10]\n",
    "subjects = [21]\n",
    "#recorded eeg class\n",
    "eeg = EEG(path, subjects, runs)\n",
    "raw=eeg.data_to_raw()\n",
    "\n",
    "print(\"Raw done\")\n",
    "# apply filter \n",
    "#iir_param = dict(order=5, ftype='butter', output='sos')\n",
    "#raw=raw.notch_filter([50])\n",
    "#raw=raw.filter(8,14,method = 'iir',iir_params=iir_param,phase='zero')\n",
    "#raw.filter(8,13, method='fir', verbose=\"error\")\n",
    "#print(\"Filter done\")\n",
    "#raw=eeg.raw_ica()\n",
    "#eeg.create_epochs()\n",
    "data, sf = raw.get_data(), raw.info['sfreq']\n",
    "\n",
    "print(data.shape)\n",
    "print(sf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw = eeg.set_reference(raw,['CZ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pick channel\n",
    "# print(raw.info['ch_names'])\n",
    "# raw = eeg.pickChannel(raw,['C3','C4','STIM MARKERS'])\n",
    "# print(raw.info['ch_names'])\n",
    "# print(len(raw.info['ch_names']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3231, 7984, 12739, 17464, 22218, 26972, 31681, 36435, 41189, 45943]\n"
     ]
    }
   ],
   "source": [
    "X,y = eeg.raw_preprocess(raw,event_id=[2.0],rest_stage=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective window size : 1.024 (s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pongkornsettasompop/anaconda3/envs/mi/lib/python3.10/site-packages/mne/time_frequency/multitaper.py:151: RuntimeWarning: invalid value encountered in divide\n",
      "  d_k = psd_iter / (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average band power3924.3429399626975\n",
      "Multitaper bandnan\n"
     ]
    }
   ],
   "source": [
    "#Feature extraction\n",
    "bp = bandpower(X, sf, [8, 13])\n",
    "mul_bp = bandpower(X,sf,[8,13],method='multitaper')\n",
    "print('Average band power'+ str(bp))\n",
    "print('Multitaper band'+ str(mul_bp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 8, 1750)\n",
      "[-0.40787173  0.23934568  0.40821856 ...  1.68567048  1.00960541\n",
      "  0.53274711]\n",
      "[1 1 1 1 1 1 1 1 1 1]\n",
      "[-0.3540237  39.38733037 -8.32761988 -9.95921699  0.          0.\n",
      "  0.          0.        ]\n",
      "(10, 8, 1750)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(X[0,0,:])\n",
    "print(y)\n",
    "\n",
    "X = eeg.apply_baseline(X)\n",
    "X = np.transpose(X,(0,2,1))\n",
    "print(X[0,0,:])\n",
    "print(np.transpose(X,(0,2,1)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epochs=eeg.epochs(raw,tmin=-2,tmax=5,baseline=(-2,0))\n",
    "print(len(epochs.times))\n",
    "#X = X[:, :,np.newaxis,:]\n",
    "X, y = eeg.get_X_y(epochs)\n",
    "#(250*5)\n",
    "#normal version \n",
    "#X = X[:,:,(250*3):]\n",
    "\n",
    "#new version\n",
    "#X = X[:,:,(250*8):(250*13)]\n",
    "\n",
    "print(X.shape)\n",
    "print(type(X))\n",
    "#new_X = np.concatenate((fixation_X,imagine_X),axis=2)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Old version use X. New version use new_X\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,stratify=y)\n",
    "print('Train size',X_train.shape, y_train.shape)\n",
    "print('Test size',X_test.shape, y_test.shape)\n",
    "print(y_train)\n",
    "print(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1750\n",
    "\n",
    "train_loader = create_dataloader(X_train, y_train, batch_size=batch_size)\n",
    "test_loader = create_dataloader(X_test, y_test, batch_size=batch_size)\n",
    "\n",
    "num_step =math.ceil(len(train_loader.dataset) / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wand setup\n",
    "#\"weightname\":\"S12_3-9_fir8-14\"\n",
    "#name=f\"CNN_S12_3-9_fir8-14\",\n",
    "wandb.login()\n",
    "wand = wandb.init(\n",
    "      # Set the project where this run will be logged\n",
    "      project=\"Motor-Imagery-New\", \n",
    "      # We pass a run name (otherwise itâ€™ll be randomly assigned, like sunshine-lollypop-10)\n",
    "      name=f\"TransR2_S17_iir_-2-5_8-13_3ch\", \n",
    "      # Track hyperparameters and run metadata\n",
    "      # 0.0000001\n",
    "      config={\n",
    "      \"learning_rate\": 0.0000001,\n",
    "      \"architecture\": \"ConvNet\",\n",
    "      \"dataset\": \"Recorded\",\n",
    "      \"epochs\": 100000,\n",
    "      \"weightname\":\"S17_TransformeR2_iir_-2-5_8-13_3ch\",\n",
    "      \"num_step_per_epoch\" : num_step, \n",
    "      }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import ConvNet,CNN2D,gamenet,ConvNet2\n",
    "from torchsummary import summary\n",
    "from transformer import Transformer\n",
    "# net = ConvNet2()\n",
    "\n",
    "device = 'cuda'\n",
    "sequence_len=1750 # sequence length of time series\n",
    "max_len=5000 # max time series sequence length \n",
    "n_head = 4 # number of attention head\n",
    "n_layer = 2 # number of encoder layer\n",
    "drop_prob = 0.1\n",
    "d_model = 200 # number of dimension ( for positional embedding)\n",
    "ffn_hidden = 512 # size of hidden layer before classification \n",
    "in_features = 3 # for univariate time series (1d), it must be adjusted for 1. \n",
    "n_classes = 2\n",
    "net =  Transformer( in_features=in_features,\n",
    "                     d_model=d_model,\n",
    "                     details=False,\n",
    "                     n_head=n_head,\n",
    "                     max_len=max_len,\n",
    "                     seq_len=sequence_len,\n",
    "                     ffn_hidden=ffn_hidden,\n",
    "                     n_layers=n_layer,\n",
    "                     drop_prob=drop_prob,\n",
    "                     n_classes=n_classes,\n",
    "                     device=device\n",
    "                     )\n",
    "#summary(net, (2, 641),32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import train\n",
    "config = wand.config\n",
    "optimizer = optim.Adam(net.parameters(), lr=config.learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = 'cuda'\n",
    "\n",
    "train_loss,valid_loss,train_accuracy,valid_accuracy =train(\n",
    "    model = net,\n",
    "    loader_train = train_loader,\n",
    "    loader_test = test_loader,\n",
    "    vail_loader = None,\n",
    "    optimizer = optimizer  ,\n",
    "    criterion = criterion ,\n",
    "    device = 'cuda',\n",
    "    wand = wand\n",
    ")\n",
    "\n",
    "\n",
    "wandb.alert(\n",
    "            title='Finish',\n",
    "            text=f'Finishing training',\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torchsummary import summary\n",
    "from transformer import Transformer\n",
    " \n",
    "\n",
    "sequence_len=1750 # sequence length of time series\n",
    "max_len=5000 # max time series sequence length \n",
    "n_head = 4 # number of attention head\n",
    "n_layer = 2 # number of encoder layer\n",
    "drop_prob = 0.1\n",
    "d_model = 200 # number of dimension ( for positional embedding)\n",
    "ffn_hidden = 512 # size of hidden layer before classification \n",
    "in_features = 3 # for univariate time series (1d), it must be adjusted for 1. \n",
    "n_classes = 2\n",
    "model =  Transformer( in_features=in_features,\n",
    "                     d_model=d_model,\n",
    "                     details=False,\n",
    "                     n_head=n_head,\n",
    "                     max_len=max_len,\n",
    "                     seq_len=sequence_len,\n",
    "                     ffn_hidden=ffn_hidden,\n",
    "                     n_layers=n_layer,\n",
    "                     drop_prob=drop_prob,\n",
    "                     n_classes=n_classes,\n",
    "                     device=device)\n",
    "\n",
    "batch_size = 555\n",
    "\n",
    "#summary(net, (2, 641),32)\n",
    "#summary(model, input_size=(batch_size,sequence_len,feature))\n",
    "\n",
    "input_ = torch.from_numpy(np.empty((batch_size,sequence_len,in_features))).float()\n",
    "\n",
    "model(input_)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "71ee62090f476f7f208daa0d546a5a64db59508b1a22febc715667ce49424855"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
